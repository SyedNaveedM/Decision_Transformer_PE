{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe4cdba4"
      },
      "source": [
        "## 1. Installs and Imports\n",
        "This notebook implements a Decision Transformer for the `LunarLander-v3` environment, using a pre-generated offline dataset and a CNN to encode the initial visual state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import collections\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Parameters\n",
        "Parameters for the environment, training, and model architecture. **You must set the `dataset_path` variable to point to your dataset file.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# --- Environment and Dataset Parameters ---\n",
        "env_name = 'LunarLander-v3'\n",
        "# ðŸ”´ SET THIS HYPERPARAMETER to the path of your generated dataset file\n",
        "dataset_path = 'TRAJECTORIES/trajectories.pkl'\n",
        "\n",
        "rtg_target = 100                \n",
        "rtg_scale = 100                 # Scale to normalize returns to go\n",
        "\n",
        "# --- Evaluation Parameters ---\n",
        "max_eval_ep_len = 1000      # Max length of one evaluation episode\n",
        "num_eval_ep = 10            # Number of evaluation episodes per iteration\n",
        "\n",
        "# --- Training Parameters ---\n",
        "batch_size = 64             # Training batch size\n",
        "lr = 1e-4                   # Learning rate\n",
        "wt_decay = 1e-4             # Weight decay\n",
        "warmup_steps = 5000         # Warmup steps for lr scheduler\n",
        "max_train_iters = 200       # Try 500 \n",
        "num_updates_per_iter = 100  # Try 1000\n",
        "\n",
        "# --- Model Parameters ---\n",
        "context_len = 20        # K in decision transformer (can try changing to 50)\n",
        "n_blocks = 3            # Num of transformer blocks\n",
        "embed_dim = 256         # Embedding (hidden) dim of transformer (try changing to 256)\n",
        "n_heads = 1             # Num of transformer heads (8 = previous)\n",
        "dropout_p = 0.1         # Dropout probability\n",
        "vision_dim = 256          # Dimension of the CNN output vector\n",
        "\n",
        "# --- Logging and Device ---\n",
        "log_dir = \"./dt_runs_lunarlander_vision/\"\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vision and Transformer Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNTo128DVector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNTo128DVector, self).__init__()\n",
        "        \n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # (3, 200, 600) -> (16, 100, 300)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # (32, 50, 150)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # (64, 25, 75)\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((4, 4))                          # (64, 4, 4)\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),                                         # (64*4*4 = 1024)\n",
        "            nn.Linear(1024, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128)                                     # Final 256-D output\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # PyTorch expects input as (B, C, H, W)\n",
        "        x = x.permute(0, 3, 1, 2) \n",
        "        x = self.cnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class MaskedCausalAttention(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.max_T = max_T\n",
        "        self.q_net = nn.Linear(h_dim, h_dim)\n",
        "        self.k_net = nn.Linear(h_dim, h_dim)\n",
        "        self.v_net = nn.Linear(h_dim, h_dim)\n",
        "        self.proj_net = nn.Linear(h_dim, h_dim)\n",
        "        self.att_drop = nn.Dropout(drop_p)\n",
        "        self.proj_drop = nn.Dropout(drop_p)\n",
        "        ones = torch.ones((max_T, max_T))\n",
        "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
        "        self.register_buffer('mask',mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        N, D = self.n_heads, C // self.n_heads\n",
        "        q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
        "        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
        "        normalized_weights = F.softmax(weights, dim=-1)\n",
        "        attention = self.att_drop(normalized_weights @ v)\n",
        "        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
        "        out = self.proj_drop(self.proj_net(attention))\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
        "        self.mlp = nn.Sequential(\n",
        "                nn.Linear(h_dim, 4*h_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(4*h_dim, h_dim),\n",
        "                nn.Dropout(drop_p),\n",
        "            )\n",
        "        self.ln1 = nn.LayerNorm(h_dim)\n",
        "        self.ln2 = nn.LayerNorm(h_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(x)\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mlp(x)\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "class DecisionTransformer(nn.Module):\n",
        "    def __init__(self, state_dim, act_dim, vision_dim, n_blocks, h_dim, context_len,\n",
        "                 n_heads, drop_p, max_timestep=4096):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.vision_dim = vision_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.context_len = context_len\n",
        "\n",
        "        # The new input sequence length for the attention mechanism is 1 (vision) + 3*(K-1) (r,s,a)\n",
        "        input_seq_len = 1 + 3 * (context_len - 1)\n",
        "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n",
        "        self.transformer = nn.Sequential(*blocks)\n",
        "\n",
        "        self.embed_ln = nn.LayerNorm(h_dim)\n",
        "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
        "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
        "        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
        "        self.embed_action = torch.nn.Embedding(act_dim, h_dim)# New embedding layer for the 8-D vision vector\n",
        "        self.embed_vision = torch.nn.Linear(vision_dim, h_dim)\n",
        "        # Positional embedding for the vision token (at position 0)\n",
        "        self.pos_emb_vision = nn.Parameter(torch.zeros(1, 1, h_dim))\n",
        "\n",
        "        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n",
        "        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n",
        "        self.predict_action = nn.Linear(h_dim, act_dim)\n",
        "\n",
        "    def forward(self, vision_vectors, timesteps, states, actions, returns_to_go):\n",
        "        B, T, _ = states.shape # Here T is context_len - 1\n",
        "        \n",
        "        # Embed the 8-D vision vector and add its unique positional embedding\n",
        "        vision_embedding = self.embed_vision(vision_vectors).unsqueeze(1) + self.pos_emb_vision\n",
        "\n",
        "        # Embed trajectory sequence (r, s, a) for T steps\n",
        "        time_embeddings = self.embed_timestep(timesteps)\n",
        "        state_embeddings = self.embed_state(states) + time_embeddings\n",
        "        action_embeddings = self.embed_action(actions) + time_embeddings\n",
        "        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n",
        "\n",
        "        # Reshape trajectory embeddings so we can interleave them\n",
        "        h_traj = torch.stack(\n",
        "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
        "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
        "\n",
        "        # Concatenate the vision token at the start of the sequence\n",
        "        h = torch.cat((vision_embedding, h_traj), dim=1)\n",
        "        h = self.embed_ln(h)\n",
        "\n",
        "        # Transformer forward pass\n",
        "        h = self.transformer(h)\n",
        "\n",
        "        # Predictions\n",
        "        # The indices are shifted by 1 because of the vision token\n",
        "        # h[:, 0] is vision, h[:, 1] is r_0, h[:, 2] is s_0, etc.\n",
        "        # Predict action given vision, r_t, s_t\n",
        "        action_preds = self.predict_action(h[:, 2::3])\n",
        "        # Predict state given vision, r_t, s_t, a_t\n",
        "        state_preds = self.predict_state(h[:, 3::3])\n",
        "        # Predict rtg given vision, r_t, s_t, a_t\n",
        "        return_preds = self.predict_rtg(h[:, 3::3])\n",
        "\n",
        "        return state_preds, action_preds, return_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Utilities and Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discount_cumsum(x, gamma):\n",
        "    disc_cumsum = np.zeros_like(x)\n",
        "    disc_cumsum[-1] = x[-1]\n",
        "    for t in reversed(range(x.shape[0]-1)):\n",
        "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
        "    return disc_cumsum\n",
        "\n",
        "def compute_dataset_stats(trajectories):\n",
        "    all_states = np.concatenate([traj['states'] for traj in trajectories], axis=0)\n",
        "    state_mean = np.mean(all_states, axis=0)\n",
        "    state_std = np.std(all_states, axis=0) + 1e-6\n",
        "    return state_mean, state_std\n",
        "\n",
        "class TrajectoryDataset(Dataset):\n",
        "    def __init__(self, trajectories, context_len, rtg_scale, state_mean, state_std):\n",
        "        self.context_len = context_len\n",
        "        self.state_mean = torch.from_numpy(state_mean).float()\n",
        "        self.state_std = torch.from_numpy(state_std).float()\n",
        "        self.trajectories = []\n",
        "\n",
        "        for traj in trajectories:\n",
        "            obs_norm = (torch.from_numpy(traj['states']).float() - self.state_mean) / self.state_std\n",
        "            rtg = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n",
        "            \n",
        "            self.trajectories.append({\n",
        "                'observations': obs_norm,\n",
        "                'actions': torch.from_numpy(traj['actions']).long(),\n",
        "                'returns_to_go': torch.from_numpy(rtg).float(),\n",
        "                'vision_vector': torch.from_numpy(traj['vision_vector']).float()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trajectories)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        traj = self.trajectories[idx]\n",
        "        traj_len = traj['observations'].shape[0]\n",
        "        \n",
        "        # The vision vector is the same for the whole trajectory\n",
        "        vision_vector = traj['vision_vector']\n",
        "\n",
        "        # We sample a sequence of K-1 triplets\n",
        "        si = random.randint(0, traj_len - 1)\n",
        "        \n",
        "        # Note: The context for (r,s,a) is now context_len - 1\n",
        "        k_minus_1 = self.context_len - 1\n",
        "        \n",
        "        states = traj['observations'][si : si + k_minus_1]\n",
        "        actions = traj['actions'][si : si + k_minus_1]\n",
        "        returns_to_go = traj['returns_to_go'][si : si + k_minus_1].unsqueeze(-1)\n",
        "        timesteps = torch.arange(si, si + states.shape[0])\n",
        "\n",
        "        # Padding\n",
        "        tlen = states.shape[0]\n",
        "        states = torch.cat([states, torch.zeros((k_minus_1 - tlen, states.shape[1]))], 0)\n",
        "        actions = torch.cat([actions, torch.zeros(k_minus_1 - tlen, dtype=torch.long)], 0)\n",
        "        returns_to_go = torch.cat([returns_to_go, torch.zeros((k_minus_1 - tlen, 1))], 0)\n",
        "        timesteps = torch.cat([timesteps, torch.zeros(k_minus_1 - tlen, dtype=torch.long)], 0)\n",
        "        traj_mask = torch.cat([torch.ones(tlen, dtype=torch.long), torch.zeros(k_minus_1 - tlen, dtype=torch.long)], 0)\n",
        "\n",
        "        return vision_vector, timesteps, states, actions, returns_to_go, traj_mask\n",
        "\n",
        "# class TrajectoryDataset(Dataset):\n",
        "#     def __init__(self, trajectories, context_len, rtg_scale, state_mean, state_std):\n",
        "#         self.context_len = context_len\n",
        "#         self.state_mean = torch.from_numpy(state_mean).float()\n",
        "#         self.state_std = torch.from_numpy(state_std).float()\n",
        "#         self.trajectories = []\n",
        "\n",
        "#         for traj in trajectories:\n",
        "#             obs_norm = (torch.from_numpy(traj['states']).float() - self.state_mean) / self.state_std\n",
        "#             rtg = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n",
        "\n",
        "#             self.trajectories.append({\n",
        "#                 'observations': obs_norm,\n",
        "#                 'actions': torch.from_numpy(traj['actions']).long(),\n",
        "#                 'returns_to_go': torch.from_numpy(rtg).float(),\n",
        "#                 'vision_vector': torch.from_numpy(traj['vision_vector']).float()\n",
        "#             })\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.trajectories)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         traj = self.trajectories[idx]\n",
        "#         traj_len = traj['observations'].shape[0]\n",
        "\n",
        "#         k_minus_1 = self.context_len - 1\n",
        "#         si = random.randint(0, traj_len - 1)\n",
        "\n",
        "#         states = traj['observations'][si : si + k_minus_1]\n",
        "#         actions = traj['actions'][si : si + k_minus_1]\n",
        "#         returns_to_go = traj['returns_to_go'][si : si + k_minus_1].unsqueeze(-1)\n",
        "#         timesteps = torch.arange(si, si + states.shape[0])\n",
        "\n",
        "#         # Padding if trajectory is too short\n",
        "#         tlen = states.shape[0]\n",
        "#         states = torch.cat([states, torch.zeros((k_minus_1 - tlen, states.shape[1]))], 0)\n",
        "#         actions = torch.cat([actions, torch.zeros(k_minus_1 - tlen, dtype=torch.long)], 0)\n",
        "#         returns_to_go = torch.cat([returns_to_go, torch.zeros((k_minus_1 - tlen, 1))], 0)\n",
        "#         timesteps = torch.cat([timesteps, torch.zeros(k_minus_1 - tlen, dtype=torch.long)], 0)\n",
        "#         traj_mask = torch.cat([torch.ones(tlen, dtype=torch.long), torch.zeros(k_minus_1 - tlen, dtype=torch.long)], 0)\n",
        "\n",
        "#         # ðŸ” NEW: Repeat vision_vector for each position in context_len\n",
        "#         # We now return a (context_len, vision_dim) tensor\n",
        "#         vision_vector = traj['vision_vector'].unsqueeze(0).repeat(self.context_len, 1)\n",
        "\n",
        "#         return vision_vector, timesteps, states, actions, returns_to_go, traj_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Error: Dataset file not found at 'TRAJECTORIES/trajectories.pkl'\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "class DatasetAnalyzer:\n",
        "    \"\"\"\n",
        "    A class to load and analyze an offline RL dataset of trajectories.\n",
        "\n",
        "    It calculates and displays key statistics about rewards, trajectory lengths,\n",
        "    actions, and state dimensions.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): The full path to the .pkl dataset file.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.trajectories = self._load_dataset()\n",
        "        if self.trajectories:\n",
        "            self.stats = self._calculate_stats()\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        \"\"\"Loads the dataset from the pickle file.\"\"\"\n",
        "        try:\n",
        "            with open(self.dataset_path, 'rb') as f:\n",
        "                trajectories = pickle.load(f)\n",
        "            print(f\"âœ… Successfully loaded {len(trajectories)} trajectories from '{self.dataset_path}'\")\n",
        "            return trajectories\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âŒ Error: Dataset file not found at '{self.dataset_path}'\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while loading the dataset: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _calculate_stats(self):\n",
        "        \"\"\"Calculates all the statistics for the loaded dataset.\"\"\"\n",
        "        if not self.trajectories:\n",
        "            return {}\n",
        "\n",
        "        all_rewards = [np.sum(traj['rewards']) for traj in self.trajectories]\n",
        "        all_lengths = [len(traj['states']) for traj in self.trajectories]\n",
        "        all_actions = np.concatenate([traj['actions'] for traj in self.trajectories])\n",
        "        all_states = np.concatenate([traj['states'] for traj in self.trajectories])\n",
        "\n",
        "        stats = {\n",
        "            'total_trajectories': len(self.trajectories),\n",
        "            'total_timesteps': sum(all_lengths),\n",
        "            'reward_stats': {\n",
        "                'mean': np.mean(all_rewards),\n",
        "                'std': np.std(all_rewards),\n",
        "                'min': np.min(all_rewards),\n",
        "                'max': np.max(all_rewards),\n",
        "                'all': all_rewards\n",
        "            },\n",
        "            'length_stats': {\n",
        "                'mean': np.mean(all_lengths),\n",
        "                'std': np.std(all_lengths),\n",
        "                'min': np.min(all_lengths),\n",
        "                'max': np.max(all_lengths),\n",
        "                'all': all_lengths\n",
        "            },\n",
        "            'action_stats': {\n",
        "                'distribution': collections.Counter(all_actions),\n",
        "                'unique_actions': sorted(list(np.unique(all_actions)))\n",
        "            },\n",
        "            'state_stats': {\n",
        "                'dimension': all_states.shape[1],\n",
        "                'mean': np.mean(all_states, axis=0),\n",
        "                'std': np.std(all_states, axis=0),\n",
        "                'min': np.min(all_states, axis=0),\n",
        "                'max': np.max(all_states, axis=0)\n",
        "            }\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def display_summary(self):\n",
        "        \"\"\"Prints a formatted summary of all calculated statistics and shows plots.\"\"\"\n",
        "        if not self.trajectories or not self.stats:\n",
        "            print(\"Cannot display summary. Dataset not loaded or stats not calculated.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"          DATASET STATISTICS SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # General Stats\n",
        "        print(f\"\\n--- GENERAL ---\")\n",
        "        print(f\"Total Trajectories: {self.stats['total_trajectories']:,}\")\n",
        "        print(f\"Total Timesteps:    {self.stats['total_timesteps']:,}\")\n",
        "\n",
        "        # Reward Stats\n",
        "        print(f\"\\n--- REWARDS (per trajectory) ---\")\n",
        "        rs = self.stats['reward_stats']\n",
        "        print(f\"Mean:   {rs['mean']:.2f}\")\n",
        "        print(f\"Std:    {rs['std']:.2f}\")\n",
        "        print(f\"Min:    {rs['min']:.2f}\")\n",
        "        print(f\"Max:    {rs['max']:.2f}\")\n",
        "        self._plot_reward_distribution()\n",
        "\n",
        "        # Length Stats\n",
        "        print(f\"\\n--- TRAJECTORY LENGTHS ---\")\n",
        "        ls = self.stats['length_stats']\n",
        "        print(f\"Mean:   {ls['mean']:.2f}\")\n",
        "        print(f\"Std:    {ls['std']:.2f}\")\n",
        "        print(f\"Min:    {ls['min']}\")\n",
        "        print(f\"Max:    {ls['max']}\")\n",
        "        self._plot_length_distribution()\n",
        "\n",
        "        # Action Stats\n",
        "        print(f\"\\n--- ACTIONS ---\")\n",
        "        ac_s = self.stats['action_stats']\n",
        "        print(f\"Unique Actions Found: {ac_s['unique_actions']}\")\n",
        "        print(\"Action Distribution:\")\n",
        "        for action, count in sorted(ac_s['distribution'].items()):\n",
        "            print(f\"  Action {action}: {count:,} times ({count/self.stats['total_timesteps']:.2%})\")\n",
        "        self._plot_action_distribution()\n",
        "\n",
        "        # State Stats\n",
        "        print(f\"\\n--- STATES ---\")\n",
        "        st_s = self.stats['state_stats']\n",
        "        print(f\"State Dimension: {st_s['dimension']}\")\n",
        "        print(\"\\nPer-dimension stats (mean, std, min, max):\")\n",
        "        for i in range(st_s['dimension']):\n",
        "            print(f\"  Dim {i:02d}: {st_s['mean'][i]:>8.2f}, {st_s['std'][i]:>8.2f}, {st_s['min'][i]:>8.2f}, {st_s['max'][i]:>8.2f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    def _plot_reward_distribution(self):\n",
        "        \"\"\"Plots a histogram of the trajectory rewards.\"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(self.stats['reward_stats']['all'], bins=50, color='skyblue', edgecolor='black')\n",
        "        plt.title('Reward Distribution per Trajectory')\n",
        "        plt.xlabel('Total Reward')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.grid(axis='y', alpha=0.75)\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_length_distribution(self):\n",
        "        \"\"\"Plots a histogram of the trajectory lengths.\"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.hist(self.stats['length_stats']['all'], bins=50, color='salmon', edgecolor='black')\n",
        "        plt.title('Trajectory Length Distribution')\n",
        "        plt.xlabel('Number of Timesteps')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.grid(axis='y', alpha=0.75)\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_action_distribution(self):\n",
        "        \"\"\"Plots a bar chart of the action distribution.\"\"\"\n",
        "        dist = self.stats['action_stats']['distribution']\n",
        "        actions = sorted(dist.keys())\n",
        "        counts = [dist[a] for a in actions]\n",
        "        \n",
        "        # Mapping for LunarLander-v2 actions\n",
        "        action_labels = {0: 'Do Nothing', 1: 'Fire Left', 2: 'Fire Main', 3: 'Fire Right'}\n",
        "        labels = [action_labels.get(a, f'Action {a}') for a in actions]\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.bar(labels, counts, color='mediumseagreen', edgecolor='black')\n",
        "        plt.title('Action Distribution')\n",
        "        plt.xlabel('Action')\n",
        "        plt.ylabel('Total Count')\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# ðŸ”´ Point this to the trajectory dataset file you want to analyze\n",
        "# This should be the same path used in your training and evaluation scripts.\n",
        "try:\n",
        "    analyzer = DatasetAnalyzer(dataset_path='TRAJECTORIES/trajectories.pkl')\n",
        "    if analyzer.trajectories:\n",
        "        analyzer.display_summary()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during analysis: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "start time: 25-09-13-22-56-51\n",
            "model save path: models_256\\CNN_TRAINING_2.pt\n",
            "============================================================\n",
            "Loading dataset from: TRAJECTORIES_FLATLANDER_WITH_SEED_RUGGED/Trajectories_rugged_min_0_max_150_step_5_unique_1000.pkl\n",
            "âœ… Successfully loaded 31000 trajectories.\n",
            "\n",
            "Initializing a single environment for all tasks...\n",
            "Initializing CNN for visual vector pre-computation...\n",
            "Pre-computing vision vectors for each trajectory... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u001b[36mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 31000/31000 [02:33<00:00, 201.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Vision vector pre-computation complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Total Training Progress:   0%|\u001b[32m          \u001b[0m| 0/200 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (64x128 and 256x256)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 101\u001b[0m\n\u001b[0;32m     92\u001b[0m vision_vectors \u001b[38;5;241m=\u001b[39m vision_vectors\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     93\u001b[0m timesteps, states, actions, returns_to_go, traj_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m     timesteps\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     95\u001b[0m     states\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m     traj_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     99\u001b[0m )\n\u001b[1;32m--> 101\u001b[0m state_preds, action_preds, return_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturns_to_go\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturns_to_go\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# action_preds is now (B, K-1, act_dim)\u001b[39;00m\n\u001b[0;32m    110\u001b[0m action_preds_masked \u001b[38;5;241m=\u001b[39m action_preds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, act_dim)[traj_mask\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
            "Cell \u001b[1;32mIn[11], line 110\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[1;34m(self, vision_vectors, timesteps, states, actions, returns_to_go)\u001b[0m\n\u001b[0;32m    107\u001b[0m B, T, _ \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# Here T is context_len - 1\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Embed the 8-D vision vector and add its unique positional embedding\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m vision_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_vision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_vectors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb_vision\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Embed trajectory sequence (r, s, a) for T steps\u001b[39;00m\n\u001b[0;32m    113\u001b[0m time_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_timestep(timesteps)\n",
            "File \u001b[1;32mc:\\Users\\SYED NAVEED\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SYED NAVEED\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\SYED NAVEED\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x128 and 256x256)"
          ]
        }
      ],
      "source": [
        "# --- Full Training Script (Corrected for Clean Output) ---\n",
        "\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
        "prefix = \"dt_\" + env_name\n",
        "save_model_name = \"CNN_TRAINING_5.pt\"\n",
        "save_model_path = os.path.join(\"models\", save_model_name)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"start time: \" + start_time_str)\n",
        "print(\"model save path: \" + save_model_path)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- Load Dataset from File ---\n",
        "print(f\"Loading dataset from: {dataset_path}\")\n",
        "try:\n",
        "    with open(dataset_path, 'rb') as f:\n",
        "        trajectories = pickle.load(f)\n",
        "    print(f\"âœ… Successfully loaded {len(trajectories)} trajectories.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ Error: Dataset file not found at '{dataset_path}'\")\n",
        "    print(\"ðŸ‘‰ Please ensure the file exists and the `dataset_path` variable is correct.\")\n",
        "    raise\n",
        "\n",
        "# --- Initialize Environment for All Tasks ---\n",
        "# We create the environment ONCE with 'rgb_array' mode, as it's needed for pre-computation.\n",
        "print(\"\\nInitializing a single environment for all tasks...\")\n",
        "env = gym.make(env_name, render_mode='rgb_array')\n",
        "\n",
        "# --- Pre-compute Vision Vectors ---\n",
        "print(\"Initializing CNN for visual vector pre-computation...\")\n",
        "# CNN is modified to output 128D vector\n",
        "cnn_model = CNNTo128DVector().to(device) \n",
        "cnn_model.eval()\n",
        "\n",
        "print(\"Pre-computing vision vectors for each trajectory... \")\n",
        "# The loop now uses the single 'env' instance\n",
        "for traj in tqdm(trajectories, colour=\"cyan\"): # Changed color for visual distinction\n",
        "    # SAME SEED PRODUCES THE SAME ENV SO WE BASICALLY HAVE THE TERRAIN INFO PRESENT IN THE DATASET\n",
        "    obs, info = env.reset(seed=int(traj['seed']))\n",
        "    rendered_frame = env.render()\n",
        "    cropped_frame = rendered_frame[200:400, :, :]\n",
        "    frame_tensor = torch.from_numpy(cropped_frame.copy()).float().to(device).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        vision_vector = cnn_model(frame_tensor).squeeze(0).cpu().numpy()\n",
        "    traj['vision_vector'] = vision_vector\n",
        "\n",
        "print(\"âœ… Vision vector pre-computation complete.\")\n",
        "\n",
        "# --- Initialize Dataset and DataLoader ---\n",
        "state_mean, state_std = compute_dataset_stats(trajectories)\n",
        "train_dataset = TrajectoryDataset(trajectories, context_len, rtg_scale, state_mean, state_std)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
        "data_iter = iter(train_loader)\n",
        "\n",
        "# --- Get Env Specs and Initialize Model ---\n",
        "# We now use the SAME 'env' instance to get dimensions.\n",
        "state_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.n\n",
        "\n",
        "model = DecisionTransformer(\n",
        "    state_dim=state_dim,\n",
        "    act_dim=act_dim,\n",
        "    vision_dim=vision_dim,\n",
        "    n_blocks=n_blocks,\n",
        "    h_dim=embed_dim,\n",
        "    context_len=context_len,\n",
        "    n_heads=n_heads,\n",
        "    drop_p=dropout_p,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wt_decay)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda steps: min((steps + 1) / warmup_steps, 1))\n",
        "\n",
        "# --- Main Training Loop (Corrected) ---\n",
        "total_updates = 0\n",
        "# Assign tqdm to a variable to call its methods\n",
        "progress_bar = tqdm(range(max_train_iters), desc=\"Total Training Progress\", colour=\"green\")\n",
        "\n",
        "for i_train_iter in progress_bar:\n",
        "    log_action_losses = []\n",
        "    model.train()\n",
        "\n",
        "    for _ in range(num_updates_per_iter):\n",
        "        try:\n",
        "            vision_vectors, timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(train_loader)\n",
        "            vision_vectors, timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
        "\n",
        "        vision_vectors = vision_vectors.to(device)\n",
        "        timesteps, states, actions, returns_to_go, traj_mask = (\n",
        "            timesteps.to(device),\n",
        "            states.to(device),\n",
        "            actions.to(device),\n",
        "            returns_to_go.to(device),\n",
        "            traj_mask.to(device)\n",
        "        )\n",
        "\n",
        "        state_preds, action_preds, return_preds = model.forward(\n",
        "            vision_vectors=vision_vectors,\n",
        "            timesteps=timesteps,\n",
        "            states=states,\n",
        "            actions=actions,\n",
        "            returns_to_go=returns_to_go\n",
        "        )\n",
        "\n",
        "        # action_preds is now (B, K-1, act_dim)\n",
        "        action_preds_masked = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
        "        action_target_masked = actions.view(-1)[traj_mask.view(-1,) > 0]\n",
        "        \n",
        "        if action_target_masked.shape[0] > 0: # Avoid loss calculation on empty tensors\n",
        "            action_loss = F.cross_entropy(action_preds_masked, action_target_masked)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            action_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            log_action_losses.append(action_loss.detach().cpu().item())\n",
        "\n",
        "    total_updates += num_updates_per_iter\n",
        "    mean_action_loss = np.mean(log_action_losses) if log_action_losses else 0\n",
        "    \n",
        "    # Use set_postfix to update the loss on the progress bar itself, removing the need for print()\n",
        "    progress_bar.set_postfix({\n",
        "        \"loss\": f\"{mean_action_loss:.5f}\",\n",
        "        \"updates\": f\"{total_updates}\"\n",
        "    })\n",
        "\n",
        "    # --- Save Model and Stats Together ---\n",
        "    # The model is saved on each iteration without printing a disruptive message\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'state_mean': state_mean,\n",
        "        'state_std': state_std,\n",
        "        'loss' : f\"{mean_action_loss:.5f}\"\n",
        "    }\n",
        "    torch.save(checkpoint, save_model_path)\n",
        "\n",
        "# Close the single environment instance at the very end\n",
        "env.close()\n",
        "\n",
        "# Print a final summary after the loop is complete\n",
        "time_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
        "print(f\"\\n{'=' * 60}\")\n",
        "print(\"Finished training!\")\n",
        "print(f\"Total time elapsed: {time_elapsed}\")\n",
        "print(f\"Final model and stats saved to: {save_model_path}\")\n",
        "print(f\"{'=' * 60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\SYED NAVEED\\AppData\\Local\\Temp\\ipykernel_24728\\3202801754.py:126: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(eval_model_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Evaluation ---\n",
            "âœ… Loaded model and stats from single file: models_256/CNN_TRAINING_1.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Episodes: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 100/100 [01:06<00:00,  1.50it/s, Last Reward=257.52]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Individual Episode Rewards ---\n",
            "Reward for episode 1 : 278.56\n",
            "Reward for episode 2 : 264.05\n",
            "Reward for episode 3 : 281.16\n",
            "Reward for episode 4 : -113.69\n",
            "Reward for episode 5 : 305.04\n",
            "Reward for episode 6 : 275.71\n",
            "Reward for episode 7 : 265.14\n",
            "Reward for episode 8 : 268.28\n",
            "Reward for episode 9 : 263.77\n",
            "Reward for episode 10 : 242.52\n",
            "Reward for episode 11 : 301.59\n",
            "Reward for episode 12 : 269.20\n",
            "Reward for episode 13 : -66.99\n",
            "Reward for episode 14 : 272.04\n",
            "Reward for episode 15 : 246.34\n",
            "Reward for episode 16 : 294.04\n",
            "Reward for episode 17 : -179.75\n",
            "Reward for episode 18 : 278.20\n",
            "Reward for episode 19 : 288.76\n",
            "Reward for episode 20 : 280.86\n",
            "Reward for episode 21 : 256.01\n",
            "Reward for episode 22 : 304.75\n",
            "Reward for episode 23 : 279.96\n",
            "Reward for episode 24 : 303.57\n",
            "Reward for episode 25 : 264.59\n",
            "Reward for episode 26 : 247.59\n",
            "Reward for episode 27 : 300.48\n",
            "Reward for episode 28 : 198.80\n",
            "Reward for episode 29 : 240.20\n",
            "Reward for episode 30 : 285.89\n",
            "Reward for episode 31 : 278.17\n",
            "Reward for episode 32 : 246.13\n",
            "Reward for episode 33 : 266.26\n",
            "Reward for episode 34 : 265.51\n",
            "Reward for episode 35 : 286.95\n",
            "Reward for episode 36 : 290.22\n",
            "Reward for episode 37 : 306.06\n",
            "Reward for episode 38 : 234.30\n",
            "Reward for episode 39 : 278.41\n",
            "Reward for episode 40 : 221.12\n",
            "Reward for episode 41 : 251.34\n",
            "Reward for episode 42 : 253.18\n",
            "Reward for episode 43 : 255.34\n",
            "Reward for episode 44 : -190.61\n",
            "Reward for episode 45 : 242.90\n",
            "Reward for episode 46 : 266.25\n",
            "Reward for episode 47 : 271.57\n",
            "Reward for episode 48 : 257.78\n",
            "Reward for episode 49 : 258.78\n",
            "Reward for episode 50 : 283.47\n",
            "Reward for episode 51 : 270.77\n",
            "Reward for episode 52 : 280.74\n",
            "Reward for episode 53 : 279.54\n",
            "Reward for episode 54 : 272.28\n",
            "Reward for episode 55 : 253.14\n",
            "Reward for episode 56 : 250.98\n",
            "Reward for episode 57 : 292.85\n",
            "Reward for episode 58 : 257.52\n",
            "Reward for episode 59 : 238.64\n",
            "Reward for episode 60 : 270.64\n",
            "Reward for episode 61 : 272.17\n",
            "Reward for episode 62 : 281.76\n",
            "Reward for episode 63 : -130.47\n",
            "Reward for episode 64 : 232.09\n",
            "Reward for episode 65 : 253.76\n",
            "Reward for episode 66 : 233.39\n",
            "Reward for episode 67 : 311.53\n",
            "Reward for episode 68 : 235.50\n",
            "Reward for episode 69 : 284.02\n",
            "Reward for episode 70 : 301.84\n",
            "Reward for episode 71 : 263.11\n",
            "Reward for episode 72 : 288.98\n",
            "Reward for episode 73 : -122.79\n",
            "Reward for episode 74 : 283.57\n",
            "Reward for episode 75 : 282.08\n",
            "Reward for episode 76 : 278.46\n",
            "Reward for episode 77 : 282.48\n",
            "Reward for episode 78 : 270.85\n",
            "Reward for episode 79 : -217.43\n",
            "Reward for episode 80 : 194.54\n",
            "Reward for episode 81 : 274.90\n",
            "Reward for episode 82 : 274.95\n",
            "Reward for episode 83 : 297.51\n",
            "Reward for episode 84 : 269.62\n",
            "Reward for episode 85 : 257.93\n",
            "Reward for episode 86 : 223.02\n",
            "Reward for episode 87 : 261.00\n",
            "Reward for episode 88 : 291.68\n",
            "Reward for episode 89 : 244.75\n",
            "Reward for episode 90 : 257.34\n",
            "Reward for episode 91 : 265.23\n",
            "Reward for episode 92 : 20.09\n",
            "Reward for episode 93 : 276.85\n",
            "Reward for episode 94 : 292.77\n",
            "Reward for episode 95 : 288.04\n",
            "Reward for episode 96 : 260.01\n",
            "Reward for episode 97 : 255.07\n",
            "Reward for episode 98 : 237.85\n",
            "Reward for episode 99 : 259.79\n",
            "Reward for episode 100 : 257.52\n",
            "\n",
            "========================================\n",
            "          EVALUATION RESULTS\n",
            "========================================\n",
            "Episodes evaluated: 100\n",
            "Mean Reward: 236.32\n",
            "Standard Deviation: 110.62\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# # --- Evaluation Parameters ---\n",
        "# # ðŸ”´ Point this to the model checkpoint you want to evaluate\n",
        "# eval_model_path = \"models_256/CNN_TRAINING_1.pt\"\n",
        "\n",
        "# eval_num_episodes = 100         # Number of episodes to run for evaluation\n",
        "# eval_rtg_target = 120        # The target return-to-go for the model\n",
        "# eval_render_mode = 'rgb_array' # Use 'rgb_array' for headless evaluation\n",
        "\n",
        "# # --- The Evaluation Function ---\n",
        "\n",
        "# def evaluate_vision_model(dt_model, cnn_model, env, num_episodes, max_ep_len, context_len,\n",
        "#                           state_mean, state_std, rtg_target, rtg_scale, device):\n",
        "#     \"\"\"\n",
        "#     Evaluates the vision-based Decision Transformer model.\n",
        "#     \"\"\"\n",
        "#     dt_model.eval()\n",
        "#     cnn_model.eval()\n",
        "    \n",
        "#     total_rewards = []\n",
        "#     state_dim = env.observation_space.shape[0]\n",
        "    \n",
        "\n",
        "#     state_mean = torch.from_numpy(state_mean).to(device)\n",
        "#     state_std = torch.from_numpy(state_std).to(device)\n",
        "    \n",
        "#     # Use tqdm for a clean progress bar during evaluation\n",
        "#     progress_bar = tqdm(range(num_episodes), desc=\"Evaluating Episodes\", colour=\"blue\")\n",
        "    \n",
        "#     for i in progress_bar:\n",
        "#         ep_reward = 0\n",
        "#         obs, _ = env.reset()\n",
        "\n",
        "#         # Get the initial vision vector for the entire episode\n",
        "#         with torch.no_grad():\n",
        "#             rendered_frame = env.render()\n",
        "#             cropped_frame = rendered_frame[200:400, :, :]\n",
        "#             frame_tensor = torch.from_numpy(cropped_frame.copy()).float().to(device).unsqueeze(0)\n",
        "#             vision_vector = cnn_model(frame_tensor) # Shape: (1, vision_dim)\n",
        "\n",
        "#         # Create history buffers for the episode\n",
        "#         states = torch.zeros((1, max_ep_len, state_dim), dtype=torch.float32, device=device)\n",
        "#         actions = torch.zeros((1, max_ep_len), dtype=torch.long, device=device)\n",
        "#         rewards_to_go = torch.zeros((1, max_ep_len, 1), dtype=torch.float32, device=device)\n",
        "#         timesteps = torch.arange(max_ep_len, device=device).unsqueeze(0)\n",
        "\n",
        "#         running_rtg = rtg_target / rtg_scale\n",
        "\n",
        "#         for t in range(max_ep_len):\n",
        "#             states[0, t] = torch.from_numpy(obs).to(device)\n",
        "#             rewards_to_go[0, t] = running_rtg\n",
        "\n",
        "#             # Prepare context for the model (K-1 trajectory steps)\n",
        "#             k_minus_1 = context_len - 1\n",
        "#             start_idx = max(0, t - k_minus_1 + 1)\n",
        "#             end_idx = t + 1\n",
        "\n",
        "#             # Get the current context slices from history\n",
        "#             sliced_states = states[:, start_idx:end_idx]\n",
        "#             sliced_actions = actions[:, start_idx:end_idx]\n",
        "#             sliced_rtgs = rewards_to_go[:, start_idx:end_idx]\n",
        "#             sliced_timesteps = timesteps[:, start_idx:end_idx]\n",
        "\n",
        "#             # Pad on the left to ensure the context length is always k_minus_1\n",
        "#             pad_len = k_minus_1 - sliced_states.shape[1]\n",
        "#             padded_states = F.pad(sliced_states, (0, 0, pad_len, 0))\n",
        "#             padded_actions = F.pad(sliced_actions, (pad_len, 0), value=0)\n",
        "#             padded_rtgs = F.pad(sliced_rtgs, (0, 0, pad_len, 0))\n",
        "#             padded_timesteps = F.pad(sliced_timesteps, (pad_len, 0))\n",
        "\n",
        "#             # Normalize the state context\n",
        "#             norm_states = (padded_states - state_mean) / state_std\n",
        "\n",
        "#             with torch.no_grad():\n",
        "#                 _, action_preds, _ = dt_model.forward(\n",
        "#                     vision_vectors=vision_vector,\n",
        "#                     timesteps=padded_timesteps,\n",
        "#                     states=norm_states,\n",
        "#                     actions=padded_actions,\n",
        "#                     returns_to_go=padded_rtgs\n",
        "#                 )\n",
        "            \n",
        "#             # Select the latest action from the predictions\n",
        "#             action = torch.argmax(action_preds[0, -1]).item()\n",
        "            \n",
        "#             # Step the environment\n",
        "#             obs, reward, terminated, truncated, _ = env.step(action)\n",
        "#             done = terminated or truncated\n",
        "            \n",
        "#             # Update history and running return-to-go\n",
        "#             actions[0, t] = action\n",
        "#             ep_reward += reward\n",
        "#             running_rtg -= (reward / rtg_scale)\n",
        "            \n",
        "#             if done:\n",
        "#                 break\n",
        "        \n",
        "#         total_rewards.append(ep_reward)\n",
        "#         # Update the progress bar with the latest episode's reward\n",
        "#         progress_bar.set_postfix({\"Last Reward\": f\"{ep_reward:.2f}\"})\n",
        "\n",
        "#     return np.array(total_rewards)\n",
        "\n",
        "\n",
        "# # --- Execution ---\n",
        "# print(\"--- Starting Evaluation ---\")\n",
        "\n",
        "# # Instantiate models\n",
        "# # Assuming CNNTo128DVector is defined in a previous cell\n",
        "# eval_cnn_model = CNNTo128DVector().to(device)\n",
        "# eval_env = gym.make(env_name, render_mode=eval_render_mode)\n",
        "# eval_state_dim = eval_env.observation_space.shape[0]\n",
        "# eval_act_dim = eval_env.action_space.n\n",
        "\n",
        "# eval_dt_model = DecisionTransformer(\n",
        "#     state_dim=eval_state_dim,\n",
        "#     act_dim=eval_act_dim,\n",
        "#     vision_dim=vision_dim,\n",
        "#     n_blocks=n_blocks,\n",
        "#     h_dim=embed_dim,\n",
        "#     context_len=context_len,\n",
        "#     n_heads=n_heads,\n",
        "#     drop_p=dropout_p,\n",
        "# ).to(device)\n",
        "\n",
        "# # Load the all-in-one checkpoint\n",
        "# try:\n",
        "#     checkpoint = torch.load(eval_model_path, map_location=device)\n",
        "#     eval_dt_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     state_mean = checkpoint['state_mean']\n",
        "#     state_std = checkpoint['state_std']\n",
        "#     print(f\"âœ… Loaded model and stats from single file: {eval_model_path}\")\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"âŒ Model checkpoint not found at {eval_model_path}. Cannot proceed.\")\n",
        "#     raise\n",
        "# except KeyError:\n",
        "#     print(f\"âŒ Error: The checkpoint file at {eval_model_path} does not contain the required keys ('model_state_dict', 'state_mean', 'state_std').\")\n",
        "#     print(\"ðŸ‘‰ Please ensure you are loading a checkpoint saved with the corrected training script.\")\n",
        "#     raise\n",
        "\n",
        "\n",
        "# # Run the evaluation\n",
        "# all_rewards = evaluate_vision_model(\n",
        "#     dt_model=eval_dt_model,\n",
        "#     cnn_model=eval_cnn_model,\n",
        "#     env=eval_env,\n",
        "#     num_episodes=eval_num_episodes,\n",
        "#     max_ep_len=max_eval_ep_len,\n",
        "#     context_len=context_len,\n",
        "#     state_mean=state_mean,\n",
        "#     state_std=state_std,\n",
        "#     rtg_target=eval_rtg_target,\n",
        "#     rtg_scale=rtg_scale,\n",
        "#     device=device\n",
        "# )\n",
        "\n",
        "# eval_env.close()\n",
        "\n",
        "# # Print the individual and final results\n",
        "# print(\"\\n--- Individual Episode Rewards ---\")\n",
        "# for i, reward in enumerate(all_rewards):\n",
        "#     print(f\"Reward for episode {i+1} : {reward:.2f}\")\n",
        "\n",
        "# print(\"\\n\" + \"=\"*40)\n",
        "# print(\"          EVALUATION RESULTS\")\n",
        "# print(\"=\"*40)\n",
        "# print(f\"Episodes evaluated: {eval_num_episodes}\")\n",
        "# print(f\"Mean Reward: {np.mean(all_rewards):.2f}\")\n",
        "# print(f\"Standard Deviation: {np.std(all_rewards):.2f}\")\n",
        "# print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\SYED NAVEED\\AppData\\Local\\Temp\\ipykernel_3020\\854212865.py:148: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(eval_model_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Dual Environment Evaluation ---\n",
            "âœ… Loaded model and stats from single file: models_256/CNN_TRAINING_1.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating Episodes: 100%|\u001b[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\u001b[0m| 100/100 [08:23<00:00,  5.03s/it, Last Reward=263.07]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Individual Episode Rewards ---\n",
            "Reward for episode 1 : 265.17\n",
            "Reward for episode 2 : 293.09\n",
            "Reward for episode 3 : 252.01\n",
            "Reward for episode 4 : 287.58\n",
            "Reward for episode 5 : 295.14\n",
            "Reward for episode 6 : 226.08\n",
            "Reward for episode 7 : 300.35\n",
            "Reward for episode 8 : 292.01\n",
            "Reward for episode 9 : 268.77\n",
            "Reward for episode 10 : 268.35\n",
            "Reward for episode 11 : 274.67\n",
            "Reward for episode 12 : 249.53\n",
            "Reward for episode 13 : 298.04\n",
            "Reward for episode 14 : 300.95\n",
            "Reward for episode 15 : 244.18\n",
            "Reward for episode 16 : 246.79\n",
            "Reward for episode 17 : 263.77\n",
            "Reward for episode 18 : 223.72\n",
            "Reward for episode 19 : 244.36\n",
            "Reward for episode 20 : 253.42\n",
            "Reward for episode 21 : 260.53\n",
            "Reward for episode 22 : 261.78\n",
            "Reward for episode 23 : 221.51\n",
            "Reward for episode 24 : 235.28\n",
            "Reward for episode 25 : 222.02\n",
            "Reward for episode 26 : 254.08\n",
            "Reward for episode 27 : 235.65\n",
            "Reward for episode 28 : -138.33\n",
            "Reward for episode 29 : 260.95\n",
            "Reward for episode 30 : -182.85\n",
            "Reward for episode 31 : 313.10\n",
            "Reward for episode 32 : 277.69\n",
            "Reward for episode 33 : 282.19\n",
            "Reward for episode 34 : 295.38\n",
            "Reward for episode 35 : 279.19\n",
            "Reward for episode 36 : 257.05\n",
            "Reward for episode 37 : 278.12\n",
            "Reward for episode 38 : 273.33\n",
            "Reward for episode 39 : 257.72\n",
            "Reward for episode 40 : 273.70\n",
            "Reward for episode 41 : 318.50\n",
            "Reward for episode 42 : 295.38\n",
            "Reward for episode 43 : 262.48\n",
            "Reward for episode 44 : 273.03\n",
            "Reward for episode 45 : 256.53\n",
            "Reward for episode 46 : 272.70\n",
            "Reward for episode 47 : 241.19\n",
            "Reward for episode 48 : 289.77\n",
            "Reward for episode 49 : 282.30\n",
            "Reward for episode 50 : 245.78\n",
            "Reward for episode 51 : 255.48\n",
            "Reward for episode 52 : 305.93\n",
            "Reward for episode 53 : 297.84\n",
            "Reward for episode 54 : 58.49\n",
            "Reward for episode 55 : 284.35\n",
            "Reward for episode 56 : 280.37\n",
            "Reward for episode 57 : -183.46\n",
            "Reward for episode 58 : 252.77\n",
            "Reward for episode 59 : 251.43\n",
            "Reward for episode 60 : 254.64\n",
            "Reward for episode 61 : 286.41\n",
            "Reward for episode 62 : 274.61\n",
            "Reward for episode 63 : 276.62\n",
            "Reward for episode 64 : 248.91\n",
            "Reward for episode 65 : 264.66\n",
            "Reward for episode 66 : 274.03\n",
            "Reward for episode 67 : 312.34\n",
            "Reward for episode 68 : 257.53\n",
            "Reward for episode 69 : 295.88\n",
            "Reward for episode 70 : 279.16\n",
            "Reward for episode 71 : 279.47\n",
            "Reward for episode 72 : 288.67\n",
            "Reward for episode 73 : 258.40\n",
            "Reward for episode 74 : 184.52\n",
            "Reward for episode 75 : 234.97\n",
            "Reward for episode 76 : 248.69\n",
            "Reward for episode 77 : 278.86\n",
            "Reward for episode 78 : 265.26\n",
            "Reward for episode 79 : 286.93\n",
            "Reward for episode 80 : -93.16\n",
            "Reward for episode 81 : 303.29\n",
            "Reward for episode 82 : 293.61\n",
            "Reward for episode 83 : 253.94\n",
            "Reward for episode 84 : 253.21\n",
            "Reward for episode 85 : 292.91\n",
            "Reward for episode 86 : 277.00\n",
            "Reward for episode 87 : 268.64\n",
            "Reward for episode 88 : 257.78\n",
            "Reward for episode 89 : 262.35\n",
            "Reward for episode 90 : 276.19\n",
            "Reward for episode 91 : 281.00\n",
            "Reward for episode 92 : 304.33\n",
            "Reward for episode 93 : 284.91\n",
            "Reward for episode 94 : 285.91\n",
            "Reward for episode 95 : 264.88\n",
            "Reward for episode 96 : 243.48\n",
            "Reward for episode 97 : 272.35\n",
            "Reward for episode 98 : 262.29\n",
            "Reward for episode 99 : 238.27\n",
            "Reward for episode 100 : 263.07\n",
            "\n",
            "========================================\n",
            "          EVALUATION RESULTS\n",
            "========================================\n",
            "Episodes evaluated: 100\n",
            "Mean Reward: 250.04\n",
            "Standard Deviation: 87.49\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Evaluation Parameters ---\n",
        "# ðŸ”´ Point this to the model checkpoint you want to evaluate\n",
        "eval_model_path = \"models_256/CNN_TRAINING_1.pt\"\n",
        "\n",
        "eval_num_episodes = 100         # Number of episodes to run for evaluation\n",
        "eval_rtg_target = 120        # The target return-to-go for the model\n",
        "eval_render_mode_vision = 'rgb_array'  # For vision input\n",
        "eval_render_mode_display = 'human'     # For display\n",
        "\n",
        "# --- The Evaluation Function ---\n",
        "\n",
        "def evaluate_vision_model_dual_env(dt_model, cnn_model, env_vision, env_display, num_episodes, max_ep_len, context_len,\n",
        "                          state_mean, state_std, rtg_target, rtg_scale, device):\n",
        "    \"\"\"\n",
        "    Evaluates the vision-based Decision Transformer model using dual environments.\n",
        "    env_vision: Environment for getting RGB arrays and observations\n",
        "    env_display: Environment for human rendering (same seed as env_vision)\n",
        "    \"\"\"\n",
        "    dt_model.eval()\n",
        "    cnn_model.eval()\n",
        "    \n",
        "    total_rewards = []\n",
        "    state_dim = env_vision.observation_space.shape[0]\n",
        "    \n",
        "\n",
        "    state_mean = torch.from_numpy(state_mean).to(device)\n",
        "    state_std = torch.from_numpy(state_std).to(device)\n",
        "    \n",
        "    # Use tqdm for a clean progress bar during evaluation\n",
        "    progress_bar = tqdm(range(num_episodes), desc=\"Evaluating Episodes\", colour=\"blue\")\n",
        "    \n",
        "    for i in progress_bar:\n",
        "        ep_reward = 0\n",
        "        \n",
        "        # Generate a random seed for this episode (safe int32 range)\n",
        "        episode_seed = np.random.randint(0, 1000000)  # Simple, safe range\n",
        "        \n",
        "        # Reset both environments with the same seed\n",
        "        obs_vision, info_vision = env_vision.reset(seed=episode_seed)\n",
        "        obs_display, info_display = env_display.reset(seed=episode_seed)\n",
        "\n",
        "        # Get the initial vision vector for the entire episode from vision env\n",
        "        with torch.no_grad():\n",
        "            rendered_frame = env_vision.render()\n",
        "            cropped_frame = rendered_frame[200:400, :, :]\n",
        "            frame_tensor = torch.from_numpy(cropped_frame.copy()).float().to(device).unsqueeze(0)\n",
        "\n",
        "            # CHANGED IT TO SOME RANDOM VECTORS INSTEAD OF USING THE ACTUAL CNN\n",
        "            # vision_vector = cnn_model(frame_tensor) # Shape: (1, vision_dim)\n",
        "            vision_vector = torch.zeros(1, vision_dim, device=device)\n",
        "\n",
        "        # Create history buffers for the episode\n",
        "        states = torch.zeros((1, max_ep_len, state_dim), dtype=torch.float32, device=device)\n",
        "        actions = torch.zeros((1, max_ep_len), dtype=torch.long, device=device)\n",
        "        rewards_to_go = torch.zeros((1, max_ep_len, 1), dtype=torch.float32, device=device)\n",
        "        timesteps = torch.arange(max_ep_len, device=device).unsqueeze(0)\n",
        "\n",
        "        running_rtg = rtg_target / rtg_scale\n",
        "\n",
        "        for t in range(max_ep_len):\n",
        "            # Use observation from vision environment for decision making\n",
        "            states[0, t] = torch.from_numpy(obs_vision).to(device)\n",
        "            rewards_to_go[0, t] = running_rtg\n",
        "\n",
        "            # Prepare context for the model (K-1 trajectory steps)\n",
        "            k_minus_1 = context_len - 1\n",
        "            start_idx = max(0, t - k_minus_1 + 1)\n",
        "            end_idx = t + 1\n",
        "\n",
        "            # Get the current context slices from history\n",
        "            sliced_states = states[:, start_idx:end_idx]\n",
        "            sliced_actions = actions[:, start_idx:end_idx]\n",
        "            sliced_rtgs = rewards_to_go[:, start_idx:end_idx]\n",
        "            sliced_timesteps = timesteps[:, start_idx:end_idx]\n",
        "\n",
        "            # Pad on the left to ensure the context length is always k_minus_1\n",
        "            pad_len = k_minus_1 - sliced_states.shape[1]\n",
        "            padded_states = F.pad(sliced_states, (0, 0, pad_len, 0))\n",
        "            padded_actions = F.pad(sliced_actions, (pad_len, 0), value=0)\n",
        "            padded_rtgs = F.pad(sliced_rtgs, (0, 0, pad_len, 0))\n",
        "            padded_timesteps = F.pad(sliced_timesteps, (pad_len, 0))\n",
        "\n",
        "            # Normalize the state context\n",
        "            norm_states = (padded_states - state_mean) / state_std\n",
        "\n",
        "            with torch.no_grad():\n",
        "                _, action_preds, _ = dt_model.forward(\n",
        "                    vision_vectors=vision_vector,\n",
        "                    timesteps=padded_timesteps,\n",
        "                    states=norm_states,\n",
        "                    actions=padded_actions,\n",
        "                    returns_to_go=padded_rtgs\n",
        "                )\n",
        "            \n",
        "            # Select the latest action from the predictions\n",
        "            action = torch.argmax(action_preds[0, -1]).item()\n",
        "            \n",
        "            # Step both environments with the same action\n",
        "            obs_vision, reward_vision, terminated_vision, truncated_vision, info_vision = env_vision.step(action)\n",
        "            obs_display, reward_display, terminated_display, truncated_display, info_display = env_display.step(action)\n",
        "            \n",
        "            # Use results from vision environment for decision making\n",
        "            done = terminated_vision or truncated_vision\n",
        "            reward = reward_vision  # Use reward from vision environment\n",
        "            \n",
        "            # Update history and running return-to-go\n",
        "            actions[0, t] = action\n",
        "            ep_reward += reward\n",
        "            running_rtg -= (reward / rtg_scale)\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        total_rewards.append(ep_reward)\n",
        "        # Update the progress bar with the latest episode's reward\n",
        "        progress_bar.set_postfix({\"Last Reward\": f\"{ep_reward:.2f}\"})\n",
        "\n",
        "    return np.array(total_rewards)\n",
        "\n",
        "\n",
        "# --- Execution ---\n",
        "print(\"--- Starting Dual Environment Evaluation ---\")\n",
        "\n",
        "# Instantiate models\n",
        "# Assuming CNNTo128DVector is defined in a previous cell\n",
        "eval_cnn_model = CNNTo128DVector().to(device)\n",
        "\n",
        "# Create two environments - one for vision input, one for display\n",
        "eval_env_vision = gym.make(env_name, render_mode=eval_render_mode_vision)\n",
        "eval_env_display = gym.make(env_name, render_mode=eval_render_mode_display)\n",
        "\n",
        "eval_state_dim = eval_env_vision.observation_space.shape[0]\n",
        "eval_act_dim = eval_env_vision.action_space.n\n",
        "\n",
        "eval_dt_model = DecisionTransformer(\n",
        "    state_dim=eval_state_dim,\n",
        "    act_dim=eval_act_dim,\n",
        "    vision_dim=vision_dim,\n",
        "    n_blocks=n_blocks,\n",
        "    h_dim=embed_dim,\n",
        "    context_len=context_len,\n",
        "    n_heads=n_heads,\n",
        "    drop_p=dropout_p,\n",
        ").to(device)\n",
        "\n",
        "# Load the all-in-one checkpoint\n",
        "try:\n",
        "    checkpoint = torch.load(eval_model_path, map_location=device)\n",
        "    eval_dt_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    state_mean = checkpoint['state_mean']\n",
        "    state_std = checkpoint['state_std']\n",
        "    print(f\"âœ… Loaded model and stats from single file: {eval_model_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ Model checkpoint not found at {eval_model_path}. Cannot proceed.\")\n",
        "    raise\n",
        "except KeyError:\n",
        "    print(f\"âŒ Error: The checkpoint file at {eval_model_path} does not contain the required keys ('model_state_dict', 'state_mean', 'state_std').\")\n",
        "    print(\"ðŸ‘‰ Please ensure you are loading a checkpoint saved with the corrected training script.\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# Run the evaluation with dual environments\n",
        "all_rewards = evaluate_vision_model_dual_env(\n",
        "    dt_model=eval_dt_model,\n",
        "    cnn_model=eval_cnn_model,\n",
        "    env_vision=eval_env_vision,\n",
        "    env_display=eval_env_display,\n",
        "    num_episodes=eval_num_episodes,\n",
        "    max_ep_len=max_eval_ep_len,\n",
        "    context_len=context_len,\n",
        "    state_mean=state_mean,\n",
        "    state_std=state_std,\n",
        "    rtg_target=eval_rtg_target,\n",
        "    rtg_scale=rtg_scale,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Close both environments\n",
        "eval_env_vision.close()\n",
        "eval_env_display.close()\n",
        "\n",
        "# Print the individual and final results\n",
        "print(\"\\n--- Individual Episode Rewards ---\")\n",
        "for i, reward in enumerate(all_rewards):\n",
        "    print(f\"Reward for episode {i+1} : {reward:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"          EVALUATION RESULTS\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Episodes evaluated: {eval_num_episodes}\")\n",
        "print(f\"Mean Reward: {np.mean(all_rewards):.2f}\")\n",
        "print(f\"Standard Deviation: {np.std(all_rewards):.2f}\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
