{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe4cdba4"
      },
      "source": [
        "## 1. Installs and Imports\n",
        "This notebook implements a Decision Transformer for the `LunarLander-v3` environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import collections\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration Parameters\n",
        "Parameters for the environment, training, and model architecture. These are adapted for `LunarLander-v3` while keeping the core model parameters the same as in the original notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Environment and Dataset Parameters ---\n",
        "env_name = 'LunarLander-v3'\n",
        "rtg_target = 100               \n",
        "rtg_scale = 100                 # Scale to normalize returns to go\n",
        "num_episodes_dataset = 500      # Number of episodes to generate for the dataset\n",
        "\n",
        "# --- Evaluation Parameters ---\n",
        "max_eval_ep_len = 1000      # Max length of one evaluation episode\n",
        "num_eval_ep = 10            # Number of evaluation episodes per iteration\n",
        "\n",
        "# --- Training Parameters ---\n",
        "batch_size = 64             # Training batch size\n",
        "lr = 1e-4                   # Learning rate\n",
        "wt_decay = 1e-4             # Weight decay\n",
        "warmup_steps = 5000         # Warmup steps for lr scheduler\n",
        "max_train_iters = 200\n",
        "num_updates_per_iter = 100\n",
        "\n",
        "# --- Model Parameters ---\n",
        "context_len = 20        # K in decision transformer\n",
        "n_blocks = 3            # Num of transformer blocks\n",
        "embed_dim = 128         # Embedding (hidden) dim of transformer\n",
        "n_heads = 1             # Num of transformer heads\n",
        "dropout_p = 0.1         # Dropout probability\n",
        "\n",
        "# --- Logging and Device ---\n",
        "log_dir = \"./dt_runs_lunarlander/\"\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Generation\n",
        "Since `LunarLander-v2` doesn't have a standard offline dataset like D4RL, we generate one by running a random agent and collecting its trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset(env_name, num_episodes):\n",
        "    \"\"\"Generates a dataset of trajectories using a random policy.\"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    trajectories = []\n",
        "    print(f\"Generating a dataset of {num_episodes} episodes...\")\n",
        "\n",
        "    for i in tqdm(range(num_episodes)):\n",
        "        obs_list, act_list, rew_list, done_list = [], [], [], []\n",
        "        done = False\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            action = env.action_space.sample() # Random action\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            obs_list.append(obs)\n",
        "            act_list.append(action)\n",
        "            rew_list.append(reward)\n",
        "            done_list.append(terminated) # Use termination flag\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "        trajectories.append({\n",
        "            'observations': np.array(obs_list, dtype=np.float32),\n",
        "            'actions': np.array(act_list, dtype=np.int64),\n",
        "            'rewards': np.array(rew_list, dtype=np.float32),\n",
        "            'terminals': np.array(done_list, dtype=np.bool_)\n",
        "        })\n",
        "\n",
        "    env.close()\n",
        "    return trajectories\n",
        "\n",
        "# Generate the dataset\n",
        "trajectories = generate_dataset(env_name, num_episodes_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Decision Transformer Model\n",
        "The model architecture is adapted to handle discrete actions by using an `nn.Embedding` layer for actions and predicting action logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MaskedCausalAttention(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.max_T = max_T\n",
        "        self.q_net = nn.Linear(h_dim, h_dim)\n",
        "        self.k_net = nn.Linear(h_dim, h_dim)\n",
        "        self.v_net = nn.Linear(h_dim, h_dim)\n",
        "        self.proj_net = nn.Linear(h_dim, h_dim)\n",
        "        self.att_drop = nn.Dropout(drop_p)\n",
        "        self.proj_drop = nn.Dropout(drop_p)\n",
        "        ones = torch.ones((max_T, max_T))\n",
        "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
        "        self.register_buffer('mask',mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        N, D = self.n_heads, C // self.n_heads\n",
        "        q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
        "        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
        "        normalized_weights = F.softmax(weights, dim=-1)\n",
        "        attention = self.att_drop(normalized_weights @ v)\n",
        "        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
        "        out = self.proj_drop(self.proj_net(attention))\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
        "        self.mlp = nn.Sequential(\n",
        "                nn.Linear(h_dim, 4*h_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(4*h_dim, h_dim),\n",
        "                nn.Dropout(drop_p),\n",
        "            )\n",
        "        self.ln1 = nn.LayerNorm(h_dim)\n",
        "        self.ln2 = nn.LayerNorm(h_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(x)\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mlp(x)\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "class DecisionTransformer(nn.Module):\n",
        "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len,\n",
        "                 n_heads, drop_p, max_timestep=4096):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.h_dim = h_dim\n",
        "\n",
        "        input_seq_len = 3 * context_len\n",
        "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n",
        "        self.transformer = nn.Sequential(*blocks)\n",
        "\n",
        "        self.embed_ln = nn.LayerNorm(h_dim)\n",
        "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
        "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
        "        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
        "\n",
        "        # Use nn.Embedding for discrete actions\n",
        "        self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n",
        "\n",
        "        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n",
        "        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n",
        "        # Predict action logits for classification\n",
        "        self.predict_action = nn.Linear(h_dim, act_dim)\n",
        "\n",
        "    def forward(self, timesteps, states, actions, returns_to_go):\n",
        "        B, T, _ = states.shape\n",
        "\n",
        "        time_embeddings = self.embed_timestep(timesteps)\n",
        "        state_embeddings = self.embed_state(states) + time_embeddings\n",
        "        action_embeddings = self.embed_action(actions) + time_embeddings\n",
        "        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n",
        "\n",
        "        h = torch.stack(\n",
        "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
        "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
        "\n",
        "        h = self.embed_ln(h)\n",
        "        h = self.transformer(h)\n",
        "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        return_preds = self.predict_rtg(h[:,2])\n",
        "        state_preds = self.predict_state(h[:,2])\n",
        "        action_preds = self.predict_action(h[:,1])\n",
        "\n",
        "        return state_preds, action_preds, return_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Utilities and Dataset Class\n",
        "Helper functions for training and evaluation, and a custom `Dataset` class to handle the generated trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discount_cumsum(x, gamma):\n",
        "    disc_cumsum = np.zeros_like(x)\n",
        "    disc_cumsum[-1] = x[-1]\n",
        "    for t in reversed(range(x.shape[0]-1)):\n",
        "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
        "    return disc_cumsum\n",
        "\n",
        "def compute_dataset_stats(trajectories):\n",
        "    all_states = np.concatenate([traj['observations'] for traj in trajectories], axis=0)\n",
        "    state_mean = np.mean(all_states, axis=0)\n",
        "    state_std = np.std(all_states, axis=0) + 1e-6\n",
        "    return state_mean, state_std\n",
        "\n",
        "class TrajectoryDataset(Dataset):\n",
        "    def __init__(self, trajectories, context_len, rtg_scale, state_mean, state_std):\n",
        "        self.context_len = context_len\n",
        "        self.state_mean = torch.from_numpy(state_mean).float()\n",
        "        self.state_std = torch.from_numpy(state_std).float()\n",
        "        self.trajectories = []\n",
        "\n",
        "        for traj in trajectories:\n",
        "            # Normalize states\n",
        "            obs_norm = (torch.from_numpy(traj['observations']).float() - self.state_mean) / self.state_std\n",
        "            # Compute returns-to-go\n",
        "            rtg = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n",
        "\n",
        "            self.trajectories.append({\n",
        "                'observations': obs_norm,\n",
        "                'actions': torch.from_numpy(traj['actions']).long(),\n",
        "                'returns_to_go': torch.from_numpy(rtg).float()\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trajectories)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        traj = self.trajectories[idx]\n",
        "        traj_len = traj['observations'].shape[0]\n",
        "\n",
        "        si = random.randint(0, traj_len - 1)\n",
        "\n",
        "        states = traj['observations'][si : si + self.context_len]\n",
        "        actions = traj['actions'][si : si + self.context_len]\n",
        "        returns_to_go = traj['returns_to_go'][si : si + self.context_len].unsqueeze(-1)\n",
        "\n",
        "        # Padding\n",
        "        tlen = states.shape[0]\n",
        "        states = torch.cat([torch.zeros((self.context_len - tlen, states.shape[1])), states], 0)\n",
        "        actions = torch.cat([torch.zeros(self.context_len - tlen, dtype=torch.long), actions], 0)\n",
        "        returns_to_go = torch.cat([torch.zeros((self.context_len - tlen, 1)), returns_to_go], 0)\n",
        "\n",
        "        timesteps = torch.arange(si, si + tlen)\n",
        "        timesteps = torch.cat([torch.zeros(self.context_len - tlen, dtype=torch.long), timesteps], 0)\n",
        "        traj_mask = torch.cat([torch.zeros(self.context_len - tlen, dtype=torch.long), torch.ones(tlen, dtype=torch.long)], 0)\n",
        "\n",
        "        return timesteps, states, actions, returns_to_go, traj_mask\n",
        "\n",
        "def evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
        "                    num_eval_ep, max_test_ep_len, state_mean, state_std):\n",
        "    model.eval()\n",
        "    total_reward = 0\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    state_mean = torch.from_numpy(state_mean).to(device)\n",
        "    state_std = torch.from_numpy(state_std).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_eval_ep):\n",
        "            obs, _ = env.reset()\n",
        "            rtg = rtg_target / rtg_scale\n",
        "\n",
        "            states = torch.zeros((1, max_test_ep_len, state_dim), dtype=torch.float32, device=device)\n",
        "            actions = torch.zeros((1, max_test_ep_len), dtype=torch.long, device=device)\n",
        "            rewards_to_go = torch.zeros((1, max_test_ep_len, 1), dtype=torch.float32, device=device)\n",
        "            timesteps = torch.arange(max_test_ep_len, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "            ep_reward = 0\n",
        "            for t in range(max_test_ep_len):\n",
        "                states[0, t] = torch.from_numpy(obs).to(device)\n",
        "                states[0, t] = (states[0, t] - state_mean) / state_std\n",
        "                rewards_to_go[0, t] = rtg\n",
        "\n",
        "                start_idx = max(0, t - context_len + 1)\n",
        "                _, act_preds, _ = model.forward(timesteps[:, start_idx:t+1],\n",
        "                                                states[:, start_idx:t+1],\n",
        "                                                actions[:, start_idx:t+1],\n",
        "                                                rewards_to_go[:, start_idx:t+1])\n",
        "                \n",
        "                act = torch.argmax(act_preds[0, -1]).item()\n",
        "                actions[0, t] = act\n",
        "\n",
        "                obs, reward, terminated, truncated, _ = env.step(act)\n",
        "                done = terminated or truncated\n",
        "                ep_reward += reward\n",
        "                rtg -= (reward / rtg_scale)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "            total_reward += ep_reward\n",
        "\n",
        "    return total_reward / num_eval_ep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = datetime.now().replace(microsecond=0)\n",
        "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
        "prefix = \"dt_\" + env_name\n",
        "save_model_name = prefix + \"_model_\" + start_time_str + \".pt\"\n",
        "save_model_path = os.path.join(log_dir, save_model_name)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"start time: \" + start_time_str)\n",
        "print(\"model save path: \" + save_model_path)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- Initialize Dataset and DataLoader ---\n",
        "state_mean, state_std = compute_dataset_stats(trajectories)\n",
        "train_dataset = TrajectoryDataset(trajectories, context_len, rtg_scale, state_mean, state_std)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
        "data_iter = iter(train_loader)\n",
        "\n",
        "# --- Initialize Environment and Model ---\n",
        "env = gym.make(env_name)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.n\n",
        "\n",
        "model = DecisionTransformer(\n",
        "    state_dim=state_dim,\n",
        "    act_dim=act_dim,\n",
        "    n_blocks=n_blocks,\n",
        "    h_dim=embed_dim,\n",
        "    context_len=context_len,\n",
        "    n_heads=n_heads,\n",
        "    drop_p=dropout_p,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wt_decay)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda steps: min((steps + 1) / warmup_steps, 1))\n",
        "\n",
        "total_updates = 0\n",
        "for i_train_iter in range(max_train_iters):\n",
        "    log_action_losses = []\n",
        "    model.train()\n",
        "\n",
        "    for _ in range(num_updates_per_iter):\n",
        "        try:\n",
        "            timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(train_loader)\n",
        "            timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
        "\n",
        "        timesteps, states, actions, returns_to_go, traj_mask = (\n",
        "            timesteps.to(device),\n",
        "            states.to(device),\n",
        "            actions.to(device),\n",
        "            returns_to_go.to(device),\n",
        "            traj_mask.to(device)\n",
        "        )\n",
        "\n",
        "        state_preds, action_preds, return_preds = model.forward(\n",
        "            timesteps=timesteps,\n",
        "            states=states,\n",
        "            actions=actions,\n",
        "            returns_to_go=returns_to_go\n",
        "        )\n",
        "\n",
        "        # Apply mask and reshape for loss calculation\n",
        "        action_preds_masked = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
        "        action_target_masked = actions.view(-1)[traj_mask.view(-1,) > 0]\n",
        "\n",
        "        action_loss = F.cross_entropy(action_preds_masked, action_target_masked)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        action_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        log_action_losses.append(action_loss.detach().cpu().item())\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    eval_avg_reward = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
        "                                    num_eval_ep, max_eval_ep_len, state_mean, state_std)\n",
        "\n",
        "    mean_action_loss = np.mean(log_action_losses)\n",
        "    time_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
        "    total_updates += num_updates_per_iter\n",
        "\n",
        "    log_str = (\n",
        "        f\"\\n{'=' * 60}\\n\"\n",
        "        f\"Time elapsed: {time_elapsed}\\n\"\n",
        "        f\"Num of updates: {total_updates}\\n\"\n",
        "        f\"Action loss: {mean_action_loss:.5f}\\n\"\n",
        "        f\"Eval avg reward: {eval_avg_reward:.5f}\\n\"\n",
        "    )\n",
        "    print(log_str)\n",
        "\n",
        "    # Save model\n",
        "    print(\"Saving current model at: \" + save_model_path)\n",
        "    torch.save(model.state_dict(), save_model_path)\n",
        "\n",
        "env.close()\n",
        "print(\"\\nFinished training!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
