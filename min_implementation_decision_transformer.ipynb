{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe4cdba4"
      },
      "source": [
        "## Install minari\n",
        "\n",
        "### Subtask:\n",
        "Install the `minari` library and its dependencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Nk5Gp7hUGA"
      },
      "source": [
        "# import libs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78929193"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `gymnasium` library and its dependencies. We will also be using `pytorch` for writing the transformer and training code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSoQ3Vxx2-Wz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import collections\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7y2eIP3Detm",
        "outputId": "f6c09158-9369-4221-b810-0ef7d446b204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "namespace_metadata.json: 1.31kB [00:00, 3.25MB/s]\n",
            "metadata.json: 1.59kB [00:00, 9.76MB/s]\n",
            "namespace_metadata.json: 1.68kB [00:00, 7.83MB/s]\n",
            "namespace_metadata.json: 10.9kB [00:00, 37.7MB/s]\n",
            "namespace_metadata.json: 3.26kB [00:00, 14.1MB/s]\n",
            "namespace_metadata.json: 3.62kB [00:00, 9.66MB/s]\n",
            "namespace_metadata.json: 100% 238/238 [00:00<00:00, 1.98MB/s]\n",
            "namespace_metadata.json: 1.31kB [00:00, 6.10MB/s]\n",
            "namespace_metadata.json: 100% 110/110 [00:00<00:00, 834kB/s]\n",
            "\n",
            "Downloading mujoco/hopper/expert-v0 from Farama servers...\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "main_data.hdf5:   0% 0.00/135M [00:00<?, ?B/s]\u001b[A\n",
            "main_data.hdf5:  16% 21.0M/135M [00:00<00:00, 163MB/s]\u001b[A\n",
            "main_data.hdf5:  31% 41.9M/135M [00:00<00:00, 182MB/s]\u001b[A\n",
            "main_data.hdf5:  47% 62.9M/135M [00:00<00:00, 180MB/s]\u001b[A\n",
            "main_data.hdf5:  62% 83.9M/135M [00:00<00:00, 178MB/s]\u001b[A\n",
            "main_data.hdf5:  78% 105M/135M [00:00<00:00, 188MB/s] \u001b[A\n",
            "main_data.hdf5: 100% 135M/135M [00:00<00:00, 185MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.25it/s]\n",
            "\n",
            "Dataset mujoco/hopper/expert-v0 downloaded to /root/.minari/datasets/mujoco/hopper/expert-v0\n",
            "Observation space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Total episodes: 1086\n",
            "Total steps: 999164\n",
            "Inspecting attributes of the first 5 episodes:\n",
            "\n",
            "Episode 1:\n",
            "Available attributes:\n",
            "  - actions: <class 'numpy.ndarray'>\n",
            "  - id: <class 'numpy.int64'>\n",
            "  - infos: <class 'dict'>\n",
            "  - observations: <class 'numpy.ndarray'>\n",
            "  - rewards: <class 'numpy.ndarray'>\n",
            "  - terminations: <class 'numpy.ndarray'>\n",
            "  - truncations: <class 'numpy.ndarray'>\n",
            "\n",
            "Episode 2:\n",
            "Available attributes:\n",
            "  - actions: <class 'numpy.ndarray'>\n",
            "  - id: <class 'numpy.int64'>\n",
            "  - infos: <class 'dict'>\n",
            "  - observations: <class 'numpy.ndarray'>\n",
            "  - rewards: <class 'numpy.ndarray'>\n",
            "  - terminations: <class 'numpy.ndarray'>\n",
            "  - truncations: <class 'numpy.ndarray'>\n",
            "\n",
            "Episode 3:\n",
            "Available attributes:\n",
            "  - actions: <class 'numpy.ndarray'>\n",
            "  - id: <class 'numpy.int64'>\n",
            "  - infos: <class 'dict'>\n",
            "  - observations: <class 'numpy.ndarray'>\n",
            "  - rewards: <class 'numpy.ndarray'>\n",
            "  - terminations: <class 'numpy.ndarray'>\n",
            "  - truncations: <class 'numpy.ndarray'>\n",
            "\n",
            "Episode 4:\n",
            "Available attributes:\n",
            "  - actions: <class 'numpy.ndarray'>\n",
            "  - id: <class 'numpy.int64'>\n",
            "  - infos: <class 'dict'>\n",
            "  - observations: <class 'numpy.ndarray'>\n",
            "  - rewards: <class 'numpy.ndarray'>\n",
            "  - terminations: <class 'numpy.ndarray'>\n",
            "  - truncations: <class 'numpy.ndarray'>\n",
            "\n",
            "Episode 5:\n",
            "Available attributes:\n",
            "  - actions: <class 'numpy.ndarray'>\n",
            "  - id: <class 'numpy.int64'>\n",
            "  - infos: <class 'dict'>\n",
            "  - observations: <class 'numpy.ndarray'>\n",
            "  - rewards: <class 'numpy.ndarray'>\n",
            "  - terminations: <class 'numpy.ndarray'>\n",
            "  - truncations: <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# import gymnasium as gym\n",
        "# import minari\n",
        "# !minari download mujoco/hopper/expert-v0\n",
        "# dataset = minari.load_dataset('mujoco/hopper/expert-v0')\n",
        "# print(\"Observation space:\", dataset.observation_space)\n",
        "# print(\"Action space:\", dataset.action_space)\n",
        "# print(\"Total episodes:\", dataset.total_episodes)\n",
        "# print(\"Total steps:\", dataset.total_steps)\n",
        "# episodes = list(minari.load_dataset('mujoco/hopper/expert-v0').iterate_episodes())\n",
        "\n",
        "# # Iterate through the first few episodes and print their attributes\n",
        "# print(\"Inspecting attributes of the first 5 episodes:\")\n",
        "# for i, episode in enumerate(list(dataset.iterate_episodes())[:5]):\n",
        "#     print(f\"\\nEpisode {i+1}:\")\n",
        "#     print(\"Available attributes:\")\n",
        "#     for attr_name in dir(episode):\n",
        "#         if not attr_name.startswith('_'):\n",
        "#             try:\n",
        "#                 attr_value = getattr(episode, attr_name)\n",
        "#                 print(f\"  - {attr_name}: {type(attr_value)}\")\n",
        "#             except AttributeError:\n",
        "#                 print(f\"  - {attr_name}: [Error accessing attribute]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQcLNRgD6SaW"
      },
      "source": [
        "# training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdtDsvit6m_e",
        "outputId": "02ce8b05-25dc-48af-97a2-2db77a6ad9a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device set to:  cuda\n"
          ]
        }
      ],
      "source": [
        "# import minari\n",
        "# dataset = \"expert\"       # expert / medium / random (adjust based on available Minari datasets)\n",
        "# rtg_scale = 1000                # scale to normalize returns to go\n",
        "\n",
        "# # use v3 env for evaluation because\n",
        "# # DT paper evaluates results on v3 envs\n",
        "\n",
        "# env_name = 'Hopper-v5'\n",
        "# rtg_target = 4500\n",
        "# env_d4rl_name = 'mujoco/hopper/expert-v0'  # Now using Minari format\n",
        "\n",
        "# # env_name = 'HalfCheetah-v3'\n",
        "# # rtg_target = 6000\n",
        "# # env_d4rl_name = 'mujoco/halfcheetah/expert-v0'\n",
        "\n",
        "# # env_name = 'Walker2d-v3'\n",
        "# # rtg_target = 5000\n",
        "# # env_d4rl_name = 'mujoco/walker2d/expert-v0'\n",
        "\n",
        "# max_eval_ep_len = 1000      # max len of one evaluation episode\n",
        "# num_eval_ep = 10            # num of evaluation episodes per iteration\n",
        "\n",
        "# batch_size = 64             # training batch size\n",
        "# lr = 1e-4                   # learning rate\n",
        "# wt_decay = 1e-4             # weight decay\n",
        "# warmup_steps = 7500        # warmup steps for lr scheduler\n",
        "\n",
        "# # total updates = max_train_iters x num_updates_per_iter\n",
        "# max_train_iters = 500\n",
        "# num_updates_per_iter = 100\n",
        "\n",
        "# context_len = 20        # K in decision transformer\n",
        "# n_blocks = 3            # num of transformer blocks\n",
        "# embed_dim = 128         # embedding (hidden) dim of transformer\n",
        "# n_heads = 1             # num of transformer heads\n",
        "# dropout_p = 0.1         # dropout probability\n",
        "\n",
        "# # load data from this file\n",
        "# dataset_path = env_d4rl_name  # Now points to Minari dataset name instead of file path\n",
        "\n",
        "# # saves model and csv in this directory\n",
        "# log_dir = \"./dt_runs/\"\n",
        "\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.makedirs(log_dir)\n",
        "\n",
        "# # training and evaluation device\n",
        "# device_name = 'cuda'\n",
        "# device = torch.device(device_name)\n",
        "# print(\"device set to: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Identical hyperparameters used for the lunar lander env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Training Parameters for LunarLander-v2 ---\n",
        "\n",
        "# Data generation\n",
        "num_episodes_data = 500       # Number of episodes to generate for the offline dataset\n",
        "\n",
        "# Environment settings\n",
        "env_name = 'LunarLander-v3'\n",
        "rtg_target = 100\n",
        "# Scale rewards to a reasonable range for the model\n",
        "rtg_scale = 100\n",
        "\n",
        "# Evaluation settings\n",
        "max_eval_ep_len = 1000      # Max length of one evaluation episode\n",
        "num_eval_ep = 10            # Num of evaluation episodes per iteration\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64             # Training batch size\n",
        "lr = 1e-4                   # Learning rate\n",
        "wt_decay = 1e-4             # Weight decay\n",
        "warmup_steps = 7500         # Warmup steps for lr scheduler\n",
        "\n",
        "# Total updates = max_train_iters x num_updates_per_iter\n",
        "max_train_iters = 500\n",
        "num_updates_per_iter = 100\n",
        "\n",
        "# Model hyperparameters\n",
        "context_len = 20        # K in decision transformer\n",
        "n_blocks = 3            # Num of transformer blocks\n",
        "embed_dim = 128         # Embedding (hidden) dim of transformer\n",
        "n_heads = 1             # Num of transformer heads\n",
        "dropout_p = 0.1         # Dropout probability\n",
        "\n",
        "# Logging\n",
        "log_dir = \"./dt_runs_lunarlander/\"\n",
        "\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device set to: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNJM0LG1iziA"
      },
      "source": [
        "# decision transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHMl_Y1SicXb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "this extremely minimal GPT model is based on:\n",
        "Misha Laskin's tweet:\n",
        "https://twitter.com/MishaLaskin/status/1481767788775628801?cxt=HHwWgoCzmYD9pZApAAAA\n",
        "\n",
        "and its corresponding notebook:\n",
        "https://colab.research.google.com/drive/1NUBqyboDcGte5qAJKOl8gaJC28V_73Iv?usp=sharing\n",
        "\n",
        "the above colab has a bug while applying masked_fill which is fixed in the\n",
        "following code\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class MaskedCausalAttention(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.max_T = max_T\n",
        "        self.q_net = nn.Linear(h_dim, h_dim)\n",
        "        self.k_net = nn.Linear(h_dim, h_dim)\n",
        "        self.v_net = nn.Linear(h_dim, h_dim)\n",
        "        self.proj_net = nn.Linear(h_dim, h_dim)\n",
        "        self.att_drop = nn.Dropout(drop_p)\n",
        "        self.proj_drop = nn.Dropout(drop_p)\n",
        "        ones = torch.ones((max_T, max_T))\n",
        "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
        "        # register buffer makes sure mask does not get updated\n",
        "        # during backpropagation\n",
        "        self.register_buffer('mask',mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n",
        "        N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n",
        "        # rearrange q, k, v as (B, N, T, D)\n",
        "        q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
        "        # weights (B, N, T, T)\n",
        "        weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
        "        # causal mask applied to weights\n",
        "        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
        "        # normalize weights, all -inf -> 0 after softmax\n",
        "        normalized_weights = F.softmax(weights, dim=-1)\n",
        "        # attention (B, N, T, D)\n",
        "        attention = self.att_drop(normalized_weights @ v)\n",
        "        # gather heads and project (B, N, T, D) -> (B, T, N*D)\n",
        "        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
        "        out = self.proj_drop(self.proj_net(attention))\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
        "        self.mlp = nn.Sequential(\n",
        "                nn.Linear(h_dim, 4*h_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(4*h_dim, h_dim),\n",
        "                nn.Dropout(drop_p),\n",
        "            )\n",
        "        self.ln1 = nn.LayerNorm(h_dim)\n",
        "        self.ln2 = nn.LayerNorm(h_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention -> LayerNorm -> MLP -> LayerNorm\n",
        "        x = x + self.attention(x) # residual\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mlp(x) # residual\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecisionTransformer(nn.Module):\n",
        "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len,\n",
        "                 n_heads, drop_p, max_timestep=4096):\n",
        "        super().__init__()\n",
        "\n",
        "        self.state_dim = state_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.h_dim = h_dim\n",
        "        ### transformer blocks\n",
        "        input_seq_len = 3 * context_len\n",
        "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n",
        "        self.transformer = nn.Sequential(*blocks)\n",
        "        ### projection heads (project to embedding)\n",
        "        self.embed_ln = nn.LayerNorm(h_dim)\n",
        "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
        "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
        "        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
        "        # # discrete actions\n",
        "        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n",
        "        # use_action_tanh = False # False for discrete actions\n",
        "        # continuous actions\n",
        "        self.embed_action = torch.nn.Linear(act_dim, h_dim)\n",
        "        use_action_tanh = True # True for continuous actions\n",
        "        ### prediction heads\n",
        "        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n",
        "        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n",
        "        self.predict_action = nn.Sequential(\n",
        "            *([nn.Linear(h_dim, act_dim)] + ([nn.Tanh()] if use_action_tanh else []))\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, timesteps, states, actions, returns_to_go):\n",
        "\n",
        "        B, T, C = states.shape\n",
        "\n",
        "        time_embeddings = self.embed_timestep(timesteps)\n",
        "\n",
        "        # time embeddings are treated similar to positional embeddings\n",
        "        state_embeddings = self.embed_state(states) + time_embeddings\n",
        "        action_embeddings = self.embed_action(actions) + time_embeddings\n",
        "        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n",
        "\n",
        "        # stack rtg, states and actions and reshape sequence as\n",
        "        # (r1, s1, a1, r2, s2, a2 ...)\n",
        "        h = torch.stack(\n",
        "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
        "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
        "\n",
        "        h = self.embed_ln(h)\n",
        "\n",
        "        # transformer and prediction\n",
        "        h = self.transformer(h)\n",
        "\n",
        "        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n",
        "        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n",
        "        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n",
        "        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n",
        "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # get predictions\n",
        "        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n",
        "        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n",
        "        action_preds = self.predict_action(h[:,1])  # predict action given r, s\n",
        "\n",
        "        return state_preds, action_preds, return_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MaskedCausalAttention(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.max_T = max_T\n",
        "        self.q_net = nn.Linear(h_dim, h_dim)\n",
        "        self.k_net = nn.Linear(h_dim, h_dim)\n",
        "        self.v_net = nn.Linear(h_dim, h_dim)\n",
        "        self.proj_net = nn.Linear(h_dim, h_dim)\n",
        "        self.att_drop = nn.Dropout(drop_p)\n",
        "        self.proj_drop = nn.Dropout(drop_p)\n",
        "        ones = torch.ones((max_T, max_T))\n",
        "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        N, D = self.n_heads, C // self.n_heads\n",
        "        q = self.q_net(x).view(B, T, N, D).transpose(1, 2)\n",
        "        k = self.k_net(x).view(B, T, N, D).transpose(1, 2)\n",
        "        v = self.v_net(x).view(B, T, N, D).transpose(1, 2)\n",
        "        weights = q @ k.transpose(2, 3) / math.sqrt(D)\n",
        "        weights = weights.masked_fill(self.mask[..., :T, :T] == 0, float('-inf'))\n",
        "        normalized_weights = F.softmax(weights, dim=-1)\n",
        "        attention = self.att_drop(normalized_weights @ v)\n",
        "        attention = attention.transpose(1, 2).contiguous().view(B, T, N * D)\n",
        "        out = self.proj_drop(self.proj_net(attention))\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
        "        super().__init__()\n",
        "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(h_dim, 4 * h_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * h_dim, h_dim),\n",
        "            nn.Dropout(drop_p),\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(h_dim)\n",
        "        self.ln2 = nn.LayerNorm(h_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(x)\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mlp(x)\n",
        "        x = self.ln2(x)\n",
        "        return x\n",
        "\n",
        "class DecisionTransformer(nn.Module):\n",
        "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len,\n",
        "                 n_heads, drop_p, max_timestep=4096, is_discrete=True):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.is_discrete = is_discrete\n",
        "\n",
        "        input_seq_len = 3 * context_len\n",
        "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n",
        "        self.transformer = nn.Sequential(*blocks)\n",
        "\n",
        "        self.embed_ln = nn.LayerNorm(h_dim)\n",
        "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
        "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
        "        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
        "\n",
        "        if self.is_discrete:\n",
        "            self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n",
        "        else:\n",
        "            self.embed_action = torch.nn.Linear(act_dim, h_dim)\n",
        "\n",
        "        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n",
        "        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n",
        "        if self.is_discrete:\n",
        "            self.predict_action = nn.Linear(h_dim, act_dim)\n",
        "        else:\n",
        "            self.predict_action = nn.Sequential(\n",
        "                nn.Linear(h_dim, act_dim),\n",
        "                nn.Tanh()\n",
        "            )\n",
        "\n",
        "    def forward(self, timesteps, states, actions, returns_to_go):\n",
        "        B, T, _ = states.shape\n",
        "        time_embeddings = self.embed_timestep(timesteps)\n",
        "        state_embeddings = self.embed_state(states) + time_embeddings\n",
        "        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n",
        "\n",
        "        if self.is_discrete:\n",
        "            action_embeddings = self.embed_action(actions.squeeze(-1)) + time_embeddings\n",
        "        else:\n",
        "            action_embeddings = self.embed_action(actions) + time_embeddings\n",
        "\n",
        "        h = torch.stack(\n",
        "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
        "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
        "        h = self.embed_ln(h)\n",
        "        h = self.transformer(h)\n",
        "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        return_preds = self.predict_rtg(h[:, 2])\n",
        "        state_preds = self.predict_state(h[:, 2])\n",
        "        action_preds = self.predict_action(h[:, 1])\n",
        "\n",
        "        return state_preds, action_preds, return_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLHjV3q28LNr"
      },
      "source": [
        "# infos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btnq_IL_j4PO"
      },
      "outputs": [],
      "source": [
        "REF_MAX_SCORE = {}\n",
        "REF_MIN_SCORE = {}\n",
        "DATASET_STATS = {}\n",
        "\n",
        "def compute_dataset_stats(env_d4rl_name):\n",
        "    \"\"\"Compute statistics from Minari dataset and process trajectories in one pass\"\"\"\n",
        "    dataset = minari.load_dataset(env_d4rl_name)\n",
        "\n",
        "    all_states = []\n",
        "    all_rewards = []\n",
        "    episode_returns = []\n",
        "    trajectories = []\n",
        "\n",
        "    print(f\"Computing statistics for {env_d4rl_name}...\")\n",
        "    print(f\"Total episodes: {dataset.total_episodes}\")\n",
        "    print(f\"Total steps: {dataset.total_steps}\")\n",
        "\n",
        "    for episode in dataset.iterate_episodes():\n",
        "        # Handle episode data\n",
        "        obs = episode.observations.astype(np.float32)\n",
        "        acts = episode.actions.astype(np.float32)\n",
        "        rews = episode.rewards.astype(np.float32)\n",
        "\n",
        "        # Handle length mismatches\n",
        "        min_len = min(len(obs), len(acts), len(rews))\n",
        "\n",
        "        # Collect RAW states for normalization statistics\n",
        "        all_states.append(obs[:min_len])\n",
        "\n",
        "        # Collect rewards for min/max computation\n",
        "        all_rewards.extend(rews[:min_len])\n",
        "\n",
        "        # Compute episode return\n",
        "        episode_return = np.sum(rews[:min_len])\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "        # Create trajectory (keep RAW observations for now)\n",
        "        traj = {\n",
        "            'observations': obs[:min_len],\n",
        "            'actions': acts[:min_len],\n",
        "            'rewards': rews[:min_len],\n",
        "            'terminals': episode.terminations[:min_len].astype(np.bool_),\n",
        "            'timeouts': episode.truncations[:min_len].astype(np.bool_)\n",
        "        }\n",
        "\n",
        "        trajectories.append(traj)\n",
        "\n",
        "    # Compute state statistics on RAW data\n",
        "    all_states = np.concatenate(all_states, axis=0).astype(np.float32)\n",
        "    state_mean = np.mean(all_states, axis=0)\n",
        "    state_std = np.std(all_states, axis=0) + 1e-6\n",
        "\n",
        "    # Compute score statistics\n",
        "    episode_returns = np.array(episode_returns)\n",
        "    max_score = np.max(episode_returns)\n",
        "    min_score = np.min(episode_returns)\n",
        "\n",
        "    # Get environment key for compatibility\n",
        "    env_key = env_d4rl_name.split('/')[1]  # e.g., 'hopper' from 'mujoco/hopper/expert-v0'\n",
        "\n",
        "    # Update global dictionaries\n",
        "    REF_MAX_SCORE[env_key] = max_score\n",
        "    REF_MIN_SCORE[env_key] = min_score\n",
        "    DATASET_STATS[env_d4rl_name] = {\n",
        "        'state_mean': state_mean.tolist(),\n",
        "        'state_std': state_std.tolist()\n",
        "    }\n",
        "\n",
        "    print(f\"Environment: {env_key}\")\n",
        "    print(f\"Max score: {max_score}\")\n",
        "    print(f\"Min score: {min_score}\")\n",
        "    print(f\"Average return: {np.mean(episode_returns):.2f}\")\n",
        "    print(f\"State dim: {len(state_mean)}\")\n",
        "\n",
        "    return state_mean, state_std, min_score, max_score, trajectories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pewE01Ca4BG0"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaaymCHPlynF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "def discount_cumsum(x, gamma):\n",
        "    disc_cumsum = np.zeros_like(x)\n",
        "    disc_cumsum[-1] = x[-1]\n",
        "    for t in reversed(range(x.shape[0]-1)):\n",
        "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
        "    return disc_cumsum\n",
        "\n",
        "def get_d4rl_dataset_stats(env_d4rl_name):\n",
        "    return DATASET_STATS[env_d4rl_name]\n",
        "\n",
        "def get_d4rl_normalized_score(score, env_name):\n",
        "    env_key = env_name.split('-')[0].lower()\n",
        "    assert env_key in REF_MAX_SCORE, f'no reference score for {env_key} env to calculate d4rl score'\n",
        "    return (score - REF_MIN_SCORE[env_key]) / (REF_MAX_SCORE[env_key] - REF_MIN_SCORE[env_key])\n",
        "\n",
        "def evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
        "                    num_eval_ep=10, max_test_ep_len=1000,\n",
        "                    state_mean=None, state_std=None, render=False):\n",
        "\n",
        "    eval_batch_size = 1  # required for forward pass\n",
        "\n",
        "    results = {}\n",
        "    total_reward = 0\n",
        "    total_timesteps = 0\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "\n",
        "    if state_mean is None:\n",
        "        state_mean = torch.zeros((state_dim,)).to(device)\n",
        "    else:\n",
        "        state_mean = torch.from_numpy(state_mean).to(device)\n",
        "\n",
        "    if state_std is None:\n",
        "        state_std = torch.ones((state_dim,)).to(device)\n",
        "    else:\n",
        "        state_std = torch.from_numpy(state_std).to(device)\n",
        "\n",
        "    # same as timesteps used for training the transformer\n",
        "    # also, crashes if device is passed to arange()\n",
        "    timesteps = torch.arange(start=0, end=max_test_ep_len, step=1)\n",
        "    timesteps = timesteps.repeat(eval_batch_size, 1).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _ in range(num_eval_ep):\n",
        "\n",
        "            # zeros place holders\n",
        "            actions = torch.zeros((eval_batch_size, max_test_ep_len, act_dim),\n",
        "                                dtype=torch.float32, device=device)\n",
        "\n",
        "            states = torch.zeros((eval_batch_size, max_test_ep_len, state_dim),\n",
        "                                dtype=torch.float32, device=device)\n",
        "\n",
        "            rewards_to_go = torch.zeros((eval_batch_size, max_test_ep_len, 1),\n",
        "                                dtype=torch.float32, device=device)\n",
        "\n",
        "            # init episode\n",
        "            reset_result = env.reset()\n",
        "            if isinstance(reset_result, tuple):\n",
        "                running_state = reset_result[0]  # Gymnasium returns (obs, info)\n",
        "            else:\n",
        "                running_state = reset_result  # Old gym returns just obs\n",
        "            running_reward = 0\n",
        "            running_rtg = rtg_target / rtg_scale\n",
        "\n",
        "            for t in range(max_test_ep_len):\n",
        "\n",
        "                total_timesteps += 1\n",
        "\n",
        "                # add state in placeholder and normalize\n",
        "                states[0, t] = torch.from_numpy(running_state).to(device)\n",
        "                states[0, t] = (states[0, t] - state_mean) / state_std\n",
        "\n",
        "                # calcualate running rtg and add in placeholder\n",
        "                running_rtg = running_rtg - (running_reward / rtg_scale)\n",
        "                rewards_to_go[0, t] = running_rtg\n",
        "\n",
        "                if t < context_len:\n",
        "                    _, act_preds, _ = model.forward(timesteps[:,:context_len],\n",
        "                                                states[:,:context_len],\n",
        "                                                actions[:,:context_len],\n",
        "                                                rewards_to_go[:,:context_len])\n",
        "                    act = act_preds[0, t].detach()\n",
        "                else:\n",
        "                    _, act_preds, _ = model.forward(timesteps[:,t-context_len+1:t+1],\n",
        "                                                states[:,t-context_len+1:t+1],\n",
        "                                                actions[:,t-context_len+1:t+1],\n",
        "                                                rewards_to_go[:,t-context_len+1:t+1])\n",
        "                    act = act_preds[0, -1].detach()\n",
        "\n",
        "                step_result = env.step(act.cpu().numpy())\n",
        "                if len(step_result) == 5:  # Gymnasium returns (obs, reward, terminated, truncated, info)\n",
        "                    running_state, running_reward, terminated, truncated, _ = step_result\n",
        "                    done = terminated or truncated\n",
        "                elif len(step_result) == 4:  # Old gym returns (obs, reward, done, info)\n",
        "                    running_state, running_reward, done, _ = step_result\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected step result length: {len(step_result)}\")\n",
        "\n",
        "                # add action in placeholder\n",
        "                actions[0, t] = act\n",
        "\n",
        "                total_reward += running_reward\n",
        "\n",
        "                if render:\n",
        "                    env.render()\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "    results['eval/avg_reward'] = total_reward / num_eval_ep\n",
        "    results['eval/avg_ep_len'] = total_timesteps / num_eval_ep\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXXrs_PjAHrN"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Vb5rY_iiME"
      },
      "outputs": [],
      "source": [
        "state_mean, state_std, min_score, max_score, _ = compute_dataset_stats(env_d4rl_name)\n",
        "\n",
        "print(f\"\\nDataset: {env_d4rl_name}\")\n",
        "print(\"State mean: \", state_mean.tolist())\n",
        "print(\"State std: \", state_std.tolist())\n",
        "print(\"Min score: \", min_score)\n",
        "print(\"Max score: \", max_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo4zPTjjn0Qr"
      },
      "outputs": [],
      "source": [
        "class D4RLTrajectoryDataset(Dataset):\n",
        "    def __init__(self, dataset_path, context_len, rtg_scale):\n",
        "\n",
        "        self.context_len = context_len\n",
        "\n",
        "        # Load and process dataset in one pass (avoid double loading)\n",
        "        print(f\"Loading {dataset_path}...\")\n",
        "        state_mean_computed, state_std_computed, min_score, max_score, self.trajectories = compute_dataset_stats(dataset_path)\n",
        "\n",
        "        # Store computed statistics\n",
        "        self.state_mean = state_mean_computed.astype(np.float32)\n",
        "        self.state_std = state_std_computed.astype(np.float32)\n",
        "\n",
        "        # Process trajectories: compute RTG and normalize states\n",
        "        for traj in self.trajectories:\n",
        "            # Calculate returns to go and rescale them\n",
        "            returns_to_go = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n",
        "            traj['returns_to_go'] = returns_to_go\n",
        "\n",
        "            # Normalize states using computed statistics\n",
        "            traj['observations'] = (traj['observations'] - self.state_mean) / self.state_std\n",
        "\n",
        "        print(f\"Loaded {len(self.trajectories)} trajectories\")\n",
        "\n",
        "        # Debug: Check shapes of first trajectory\n",
        "        if len(self.trajectories) > 0:\n",
        "            first_traj = self.trajectories[0]\n",
        "            print(f\"First trajectory shapes:\")\n",
        "            print(f\"  observations: {first_traj['observations'].shape}\")\n",
        "            print(f\"  actions: {first_traj['actions'].shape}\")\n",
        "            print(f\"  returns_to_go: {first_traj['returns_to_go'].shape}\")\n",
        "\n",
        "    def get_state_stats(self):\n",
        "        return self.state_mean, self.state_std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trajectories)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        traj = self.trajectories[idx]\n",
        "        traj_len = traj['observations'].shape[0]\n",
        "\n",
        "        if traj_len >= self.context_len:\n",
        "            # sample random index to slice trajectory\n",
        "            si = random.randint(0, traj_len - self.context_len)\n",
        "\n",
        "            states = torch.from_numpy(traj['observations'][si : si + self.context_len])\n",
        "            actions = torch.from_numpy(traj['actions'][si : si + self.context_len])\n",
        "            returns_to_go = torch.from_numpy(traj['returns_to_go'][si : si + self.context_len])\n",
        "            timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n",
        "\n",
        "            # all ones since no padding\n",
        "            traj_mask = torch.ones(self.context_len, dtype=torch.long)\n",
        "\n",
        "        else:\n",
        "            padding_len = self.context_len - traj_len\n",
        "\n",
        "            # padding with zeros\n",
        "            states = torch.from_numpy(traj['observations'])\n",
        "            states = torch.cat([states,\n",
        "                                torch.zeros(([padding_len] + list(states.shape[1:])),\n",
        "                                dtype=states.dtype)],\n",
        "                               dim=0)\n",
        "\n",
        "            actions = torch.from_numpy(traj['actions'])\n",
        "            actions = torch.cat([actions,\n",
        "                                torch.zeros(([padding_len] + list(actions.shape[1:])),\n",
        "                                dtype=actions.dtype)],\n",
        "                               dim=0)\n",
        "\n",
        "            returns_to_go = torch.from_numpy(traj['returns_to_go'])\n",
        "            returns_to_go = torch.cat([returns_to_go,\n",
        "                                torch.zeros(([padding_len] + list(returns_to_go.shape[1:])),\n",
        "                                dtype=returns_to_go.dtype)],\n",
        "                               dim=0)\n",
        "\n",
        "            timesteps = torch.arange(start=0, end=self.context_len, step=1)\n",
        "\n",
        "            traj_mask = torch.cat([torch.ones(traj_len, dtype=torch.long),\n",
        "                                   torch.zeros(padding_len, dtype=torch.long)],\n",
        "                                  dim=0)\n",
        "\n",
        "        return timesteps, returns_to_go, actions, states, traj_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7AK6T9Picu-"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "RBLRM5nOVR_8",
        "outputId": "8d5e583e-1488-419b-9a28-8a9ac30f8143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "start time: 25-08-06-05-41-46\n",
            "============================================================\n",
            "device set to: cuda\n",
            "dataset path: mujoco/hopper/expert-v0\n",
            "model save path: ./dt_runs/dt_mujoco/hopper/expert-v0_model_25-08-06-05-41-46.pt\n",
            "Computing dataset statistics...\n",
            "Computing statistics for mujoco/hopper/expert-v0...\n",
            "Total episodes: 1086\n",
            "Total steps: 999164\n",
            "Environment: hopper\n",
            "Max score: 4376.3271484375\n",
            "Min score: 395.63897705078125\n",
            "Average return: 3857.80\n",
            "State dim: 11\n",
            "Loading mujoco/hopper/expert-v0...\n",
            "Computing statistics for mujoco/hopper/expert-v0...\n",
            "Total episodes: 1086\n",
            "Total steps: 999164\n",
            "Environment: hopper\n",
            "Max score: 4376.3271484375\n",
            "Min score: 395.63897705078125\n",
            "Average return: 3857.80\n",
            "State dim: 11\n",
            "Loaded 1086 trajectories\n",
            "First trajectory shapes:\n",
            "  observations: (1000, 11)\n",
            "  actions: (1000, 3)\n",
            "  returns_to_go: (1000,)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2925045671.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0maction_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         state_preds, action_preds, return_preds = model.forward(\n\u001b[0m\u001b[1;32m    105\u001b[0m                                                         \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                                                         \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-52108699.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, timesteps, states, actions, returns_to_go)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns_to_go\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mtime_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_timestep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "start_time = datetime.now().replace(microsecond=0)\n",
        "\n",
        "start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "prefix = \"dt_\" + env_d4rl_name\n",
        "\n",
        "save_model_name =  prefix + \"_model_\" + start_time_str + \".pt\"\n",
        "save_model_path = os.path.join(log_dir, save_model_name)\n",
        "save_best_model_path = save_model_path[:-3] + \"_best.pt\"\n",
        "\n",
        "log_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
        "log_csv_path = os.path.join(log_dir, log_csv_name)\n",
        "\n",
        "os.makedirs(os.path.dirname(log_csv_path), exist_ok=True)\n",
        "csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
        "csv_header = ([\"duration\", \"num_updates\", \"action_loss\",\n",
        "               \"eval_avg_reward\", \"eval_avg_ep_len\", \"eval_d4rl_score\"])\n",
        "\n",
        "csv_writer.writerow(csv_header)\n",
        "os.makedirs(os.path.dirname(save_model_path), exist_ok=True)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"start time: \" + start_time_str)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"device set to: \" + str(device))\n",
        "print(\"dataset path: \" + dataset_path)\n",
        "print(\"model save path: \" + save_model_path)\n",
        "# print(\"log csv save path: \" + log_csv_path)\n",
        "\n",
        "# Compute dataset statistics before creating dataset\n",
        "print(\"Computing dataset statistics...\")\n",
        "state_mean_computed, state_std_computed, min_score, max_score, _ = compute_dataset_stats(dataset_path)\n",
        "\n",
        "traj_dataset = D4RLTrajectoryDataset(dataset_path, context_len, rtg_scale)\n",
        "\n",
        "traj_data_loader = DataLoader(traj_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        pin_memory=True,\n",
        "                        drop_last=True)\n",
        "\n",
        "data_iter = iter(traj_data_loader)\n",
        "\n",
        "## get state stats from dataset\n",
        "state_mean, state_std = traj_dataset.get_state_stats()\n",
        "\n",
        "# Create environment - use gymnasium instead of gym for compatibility\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    env = gym.make(env_name)\n",
        "except ImportError:\n",
        "    import gym\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "\n",
        "model = DecisionTransformer(\n",
        "            state_dim=state_dim,\n",
        "            act_dim=act_dim,\n",
        "            n_blocks=n_blocks,\n",
        "            h_dim=embed_dim,\n",
        "            context_len=context_len,\n",
        "            n_heads=n_heads,\n",
        "            drop_p=dropout_p,\n",
        "        ).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "                    model.parameters(),\n",
        "                    lr=lr,\n",
        "                    weight_decay=wt_decay\n",
        "                )\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "        optimizer,\n",
        "        lambda steps: min((steps+1)/warmup_steps, 1)\n",
        "    )\n",
        "\n",
        "max_d4rl_score = -1.0\n",
        "total_updates = 0\n",
        "\n",
        "for i_train_iter in range(max_train_iters):\n",
        "\n",
        "    log_action_losses = []\n",
        "    model.train()\n",
        "\n",
        "    for _ in range(num_updates_per_iter):\n",
        "        try:\n",
        "            timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(traj_data_loader)\n",
        "            timesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
        "\n",
        "        timesteps = timesteps.to(device)    # B x T\n",
        "        states = states.to(device)          # B x T x state_dim\n",
        "        actions = actions.to(device)        # B x T x act_dim\n",
        "        returns_to_go = returns_to_go.to(device).unsqueeze(dim=-1) # B x T x 1\n",
        "        traj_mask = traj_mask.to(device)    # B x T\n",
        "\n",
        "        action_target = torch.clone(actions).detach().to(device)\n",
        "\n",
        "        state_preds, action_preds, return_preds = model.forward(\n",
        "                                                        timesteps=timesteps,\n",
        "                                                        states=states,\n",
        "                                                        actions=actions,\n",
        "                                                        returns_to_go=returns_to_go\n",
        "                                                    )\n",
        "\n",
        "        # only consider non padded elements\n",
        "        action_preds = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
        "        action_target = action_target.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
        "\n",
        "        action_loss = F.mse_loss(action_preds, action_target, reduction='mean')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        action_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        log_action_losses.append(action_loss.detach().cpu().item())\n",
        "\n",
        "    # evaluate on env\n",
        "    results = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
        "                            num_eval_ep, max_eval_ep_len, state_mean, state_std,\n",
        "                            )\n",
        "    eval_avg_reward = results['eval/avg_reward']\n",
        "    eval_avg_ep_len = results['eval/avg_ep_len']\n",
        "    eval_d4rl_score = get_d4rl_normalized_score(results['eval/avg_reward'], env_name) * 100\n",
        "\n",
        "    mean_action_loss = np.mean(log_action_losses)\n",
        "    time_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n",
        "\n",
        "    total_updates += num_updates_per_iter\n",
        "\n",
        "    log_str = (\"=\" * 60 + '\\n' +\n",
        "            \"time elapsed: \" + time_elapsed  + '\\n' +\n",
        "            \"num of updates: \" + str(total_updates) + '\\n' +\n",
        "            \"action loss: \" +  format(mean_action_loss, \".5f\") + '\\n' +\n",
        "            \"eval avg reward: \" + format(eval_avg_reward, \".5f\") + '\\n' +\n",
        "            \"eval avg ep len: \" + format(eval_avg_ep_len, \".5f\") + '\\n' +\n",
        "            \"eval d4rl score: \" + format(eval_d4rl_score, \".5f\")\n",
        "            )\n",
        "\n",
        "    print(log_str)\n",
        "\n",
        "    log_data = [time_elapsed, total_updates, mean_action_loss,\n",
        "                eval_avg_reward, eval_avg_ep_len,\n",
        "                eval_d4rl_score]\n",
        "\n",
        "    # csv_writer.writerow(log_data)\n",
        "\n",
        "    # save model\n",
        "    print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
        "    if eval_d4rl_score >= max_d4rl_score:\n",
        "        print(\"saving max d4rl score model at: \" + save_best_model_path)\n",
        "        torch.save(model.state_dict(), save_best_model_path)\n",
        "        max_d4rl_score = eval_d4rl_score\n",
        "\n",
        "    print(\"saving current model at: \" + save_model_path)\n",
        "    torch.save(model.state_dict(), save_model_path)\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"finished training!\")\n",
        "print(\"=\" * 60)\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "time_elapsed = str(end_time - start_time)\n",
        "end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
        "print(\"started training at: \" + start_time_str)\n",
        "print(\"finished training at: \" + end_time_str)\n",
        "print(\"total training time: \" + time_elapsed)\n",
        "print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n",
        "print(\"saved max d4rl score model at: \" + save_best_model_path)\n",
        "print(\"saved last updated model at: \" + save_model_path)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# csv_writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eosqWqRRJLsZ"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4-WPlC1VR3q"
      },
      "outputs": [],
      "source": [
        "eval_dataset = \"expert\"  # expert / medium / random (adjust based on available Minari datasets)\n",
        "eval_rtg_scale = 1000    # normalize returns to go\n",
        "eval_env_name = \"Hopper-v5\"\n",
        "eval_rtg_target = 5000\n",
        "eval_env_d4rl_name = 'mujoco/hopper/expert-v0'  # Minari dataset name\n",
        "\n",
        "# eval_env_name = \"HalfCheetah-v3\"\n",
        "# eval_rtg_target = 6000\n",
        "# eval_env_d4rl_name = 'mujoco/halfcheetah/expert-v0'\n",
        "\n",
        "# eval_env_name = \"Hopper-v3\"\n",
        "# eval_rtg_target = 3600\n",
        "# eval_env_d4rl_name = 'mujoco/hopper/expert-v0'\n",
        "\n",
        "num_test_eval_ep = 10        # num of evaluation episodes\n",
        "eval_max_eval_ep_len = 1000  # max len of one episode\n",
        "\n",
        "context_len = 20\n",
        "n_blocks = 3\n",
        "embed_dim = 128\n",
        "n_heads = 1\n",
        "dropout_p = 0.1\n",
        "\n",
        "eval_chk_pt_dir = \"./dt_runs/\"\n",
        "eval_chk_pt_name = \"dt_mujoco/hopper/expert-v0_model_25-07-26-22-20-35_best.pt\"  # Update with your actual checkpoint name\n",
        "eval_chk_pt_list = [eval_chk_pt_name]\n",
        "\n",
        "## manually override check point list\n",
        "## passing a list will evaluate on all checkpoints\n",
        "## and output mean and std score\n",
        "# eval_chk_pt_list = [\n",
        "#     \"dt_mujoco/hopper/expert-v0_model_25-07-26-18-59-07_best.pt\",\n",
        "#     \"dt_mujoco/hopper/expert-v0_model_25-07-26-19-15-30_best.pt\",\n",
        "#     \"dt_mujoco/hopper/expert-v0_model_25-07-26-20-45-12_best.pt\"\n",
        "# ]\n",
        "\n",
        "# Compute dataset statistics if not already computed\n",
        "# Check if the dataset stats exist, if not compute them\n",
        "if eval_env_d4rl_name not in DATASET_STATS:\n",
        "    print(f\"Computing statistics for evaluation dataset: {eval_env_d4rl_name}\")\n",
        "    compute_dataset_stats(eval_env_d4rl_name)\n",
        "\n",
        "env_data_stats = get_d4rl_dataset_stats(eval_env_d4rl_name)\n",
        "eval_state_mean = np.array(env_data_stats['state_mean'], dtype=np.float32)\n",
        "eval_state_std = np.array(env_data_stats['state_std'], dtype=np.float32)\n",
        "\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    eval_env = gym.make(eval_env_name)\n",
        "except ImportError:\n",
        "    import gym\n",
        "    eval_env = gym.make(eval_env_name)\n",
        "\n",
        "state_dim = eval_env.observation_space.shape[0]\n",
        "act_dim = eval_env.action_space.shape[0]\n",
        "\n",
        "all_scores = []\n",
        "\n",
        "for eval_chk_pt_name in eval_chk_pt_list:\n",
        "    eval_model = DecisionTransformer(\n",
        "        state_dim=state_dim,\n",
        "        act_dim=act_dim,\n",
        "        n_blocks=n_blocks,\n",
        "        h_dim=embed_dim,\n",
        "        context_len=context_len,\n",
        "        n_heads=n_heads,\n",
        "        drop_p=dropout_p,\n",
        "    ).to(device)\n",
        "\n",
        "    eval_chk_pt_path = os.path.join(eval_chk_pt_dir, eval_chk_pt_name)\n",
        "\n",
        "    eval_model.load_state_dict(torch.load(eval_chk_pt_path, map_location=device))\n",
        "    print(\"model loaded from: \" + eval_chk_pt_path)\n",
        "\n",
        "    results = evaluate_on_env(eval_model, device, context_len,\n",
        "                             eval_env, eval_rtg_target, eval_rtg_scale,\n",
        "                             num_test_eval_ep, eval_max_eval_ep_len,\n",
        "                             eval_state_mean, eval_state_std)\n",
        "    print(results)\n",
        "\n",
        "    norm_score = get_d4rl_normalized_score(results['eval/avg_reward'], eval_env_name) * 100\n",
        "    print(\"normalized d4rl score: \", norm_score)\n",
        "\n",
        "    all_scores.append(norm_score)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "all_scores = np.array(all_scores)\n",
        "print(\"evaluated on env: \" + eval_env_name)\n",
        "print(\"total num of checkpoints evaluated: \" + str(len(eval_chk_pt_list)))\n",
        "print(\"d4rl score mean: \" + format(all_scores.mean(), \".5f\"))\n",
        "print(\"d4rl score std: \" + format(all_scores.std(), \".5f\"))\n",
        "print(\"d4rl score var: \" + format(all_scores.var(), \".5f\"))\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaacQSsJJKVx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxcJqnb1Him4"
      },
      "source": [
        "## render env\n",
        "\n",
        "\n",
        "\n",
        "*   saves mp4 video of env frames and plays it in notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdf_bea2hiRs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install -U colabgymrender\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-RpNwa0hiPR"
      },
      "outputs": [],
      "source": [
        "\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "num_test_eval_ep = 1\n",
        "eval_max_ep_len = 1000\n",
        "\n",
        "\n",
        "directory = \"./render_video\"\n",
        "eval_env = Recorder(eval_env, directory)\n",
        "\n",
        "results = evaluate_on_env(eval_model, device, context_len,\n",
        "                        eval_env, eval_rtg_target, eval_rtg_scale,\n",
        "                        num_test_eval_ep, eval_max_ep_len,\n",
        "\t\t\t\t\t\teval_state_mean, eval_state_std)\n",
        "print(results)\n",
        "\n",
        "norm_score = get_d4rl_normalized_score(results['eval/avg_reward'], eval_env_name) * 100\n",
        "print(\"normalized d4rl score: \", norm_score)\n",
        "\n",
        "eval_env.play()\n",
        "\n",
        "eval_env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZNV_H78kRSL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjBsdz9mKbZg"
      },
      "source": [
        "# plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WM69ti2KaRN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "env_d4rl_name = 'mujoco/hopper/expert-v0'\n",
        "\n",
        "log_dir = 'dt_runs/'\n",
        "\n",
        "x_key = \"num_updates\"\n",
        "y_key = \"eval_d4rl_score\"\n",
        "y_smoothing_win = 5\n",
        "plot_avg = False\n",
        "save_fig = False\n",
        "\n",
        "if plot_avg:\n",
        "    save_fig_path = env_d4rl_name + \"_avg.png\"\n",
        "else:\n",
        "    save_fig_path = env_d4rl_name + \".png\"\n",
        "\n",
        "\n",
        "all_files = glob.glob(log_dir + f'/dt_{env_d4rl_name}*.csv')\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.set_title(env_d4rl_name)\n",
        "\n",
        "if plot_avg:\n",
        "    name_list = []\n",
        "    df_list = []\n",
        "    for filename in all_files:\n",
        "        frame = pd.read_csv(filename, index_col=None, header=0)\n",
        "        print(filename, frame.shape)\n",
        "        frame['y_smooth'] = frame[y_key].rolling(window=y_smoothing_win).mean()\n",
        "        df_list.append(frame)\n",
        "\n",
        "\n",
        "    df_concat = pd.concat(df_list)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    data_avg.plot(x=x_key, y='y_smooth', ax=ax)\n",
        "\n",
        "    ax.set_xlabel(x_key)\n",
        "    ax.set_ylabel(y_key)\n",
        "    ax.legend(['avg of all runs'], loc='lower right')\n",
        "\n",
        "    if save_fig:\n",
        "        plt.savefig(save_fig_path)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "else:\n",
        "    name_list = []\n",
        "    for filename in all_files:\n",
        "        frame = pd.read_csv(filename, index_col=None, header=0)\n",
        "        print(filename, frame.shape)\n",
        "        frame['y_smooth'] = frame[y_key].rolling(window=y_smoothing_win).mean()\n",
        "        frame.plot(x=x_key, y='y_smooth', ax=ax)\n",
        "        name_list.append(filename.split('/')[-1])\n",
        "\n",
        "    ax.set_xlabel(x_key)\n",
        "    ax.set_ylabel(y_key)\n",
        "    ax.legend(name_list, loc='lower right')\n",
        "\n",
        "    if save_fig:\n",
        "        plt.savefig(save_fig_path)\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
